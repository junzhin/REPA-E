# REPA-E æ ¸å¿ƒåˆ›æ–°ç‚¹ä»£ç å®žçŽ°å®Œæ•´æŒ‡å—

> **ç‰ˆæœ¬**: v1.0  
> **åˆ›å»ºæ—¶é—´**: 2025-01-13  
> **ç›®çš„**: æå–REPA-Eçš„æ‰€æœ‰æ ¸å¿ƒåˆ›æ–°å®žçŽ°ï¼Œç”¨äºŽä»£ç è¿ç§»å’Œå­¦ä¹ 

## ðŸ“‹ ç›®å½•

1. [æ¦‚è¿°å’Œæ•´ä½“æž¶æž„](#1-æ¦‚è¿°å’Œæ•´ä½“æž¶æž„)
2. [æ ¸å¿ƒæŸå¤±å‡½æ•°å®žçŽ°](#2-æ ¸å¿ƒæŸå¤±å‡½æ•°å®žçŽ°)
3. [PatchGANåˆ¤åˆ«å™¨å®Œæ•´å®žçŽ°](#3-patchganåˆ¤åˆ«å™¨å®Œæ•´å®žçŽ°)
4. [SiTæ¨¡åž‹ä¸­çš„æŠ•å½±å¯¹é½é›†æˆ](#4-sitæ¨¡åž‹ä¸­çš„æŠ•å½±å¯¹é½é›†æˆ)
5. [ç«¯åˆ°ç«¯è®­ç»ƒå¾ªçŽ¯å®žçŽ°](#5-ç«¯åˆ°ç«¯è®­ç»ƒå¾ªçŽ¯å®žçŽ°)
6. [EMAæŒ‡æ•°ç§»åŠ¨å¹³å‡æ›´æ–°æœºåˆ¶](#6-emaæŒ‡æ•°ç§»åŠ¨å¹³å‡æ›´æ–°æœºåˆ¶)
7. [Batch-normå±‚å’Œç‰¹å¾å½’ä¸€åŒ–](#7-batch-normå±‚å’Œç‰¹å¾å½’ä¸€åŒ–)
8. [è§†è§‰ç¼–ç å™¨åŠ è½½å’Œç®¡ç†ç³»ç»Ÿ](#8-è§†è§‰ç¼–ç å™¨åŠ è½½å’Œç®¡ç†ç³»ç»Ÿ)
9. [æ•°æ®é¢„å¤„ç†å’Œå½’ä¸€åŒ–å‡½æ•°](#9-æ•°æ®é¢„å¤„ç†å’Œå½’ä¸€åŒ–å‡½æ•°)
10. [é‡‡æ ·å™¨å’Œç”Ÿæˆç­–ç•¥](#10-é‡‡æ ·å™¨å’Œç”Ÿæˆç­–ç•¥)
11. [å‚æ•°é…ç½®å’Œå…³é”®è¶…å‚æ•°è®¾ç½®](#11-å‚æ•°é…ç½®å’Œå…³é”®è¶…å‚æ•°è®¾ç½®)
12. [å…³é”®å·¥å…·å‡½æ•°å’Œè¾…åŠ©ä»£ç ](#12-å…³é”®å·¥å…·å‡½æ•°å’Œè¾…åŠ©ä»£ç )
13. [é›†æˆç¤ºä¾‹å’Œä½¿ç”¨æŒ‡å—](#13-é›†æˆç¤ºä¾‹å’Œä½¿ç”¨æŒ‡å—)

---

## 1. æ¦‚è¿°å’Œæ•´ä½“æž¶æž„

### ðŸŽ¯ REPA-Eæ ¸å¿ƒåˆ›æ–°ç‚¹

REPA-E (Representation Alignment for End-to-End training) çš„ä¸»è¦åˆ›æ–°æ˜¯é€šè¿‡**æŠ•å½±å¯¹é½æŸå¤±**å®žçŽ°VAEä¸Žæ‰©æ•£æ¨¡åž‹çš„ç«¯åˆ°ç«¯è®­ç»ƒï¼Œè§£å†³äº†ä¼ ç»Ÿä¸¤é˜¶æ®µè®­ç»ƒçš„ä¼˜åŒ–é—®é¢˜ã€‚

#### æ ¸å¿ƒæŠ€æœ¯æž¶æž„ï¼š
1. **æŠ•å½±å¯¹é½æŸå¤±** - ä½¿ç”¨REPAæŸå¤±ä»£æ›¿ç›´æŽ¥çš„æ‰©æ•£æŸå¤±è¿›è¡Œç«¯åˆ°ç«¯ä¼˜åŒ–
2. **ä¸‰ä¼˜åŒ–å™¨æž¶æž„** - ç‹¬ç«‹ä¼˜åŒ–SiTã€VAEã€åˆ¤åˆ«å™¨
3. **Stop-gradientæœºåˆ¶** - é˜²æ­¢æ‰©æ•£æŸå¤±ç ´åVAEæ½œåœ¨ç©ºé—´ç»“æž„
4. **Batch-normå±‚** - è§£å†³ç«¯åˆ°ç«¯è®­ç»ƒä¸­çš„ç‰¹å¾å½’ä¸€åŒ–é—®é¢˜

#### è®­ç»ƒæµç¨‹ï¼š
```
1. VAEè®­ç»ƒ: é‡å»ºæŸå¤± + æ„ŸçŸ¥æŸå¤± + KLæŸå¤± + VAEå¯¹é½æŸå¤±
2. åˆ¤åˆ«å™¨è®­ç»ƒ: å¯¹æŠ—æŸå¤±æ›´æ–°
3. SiTè®­ç»ƒ: åŽ»å™ªæŸå¤± + SiTæŠ•å½±å¯¹é½æŸå¤± (with stop-gradient)
4. EMAæ›´æ–°: æ›´æ–°SiTçš„æŒ‡æ•°ç§»åŠ¨å¹³å‡å‚æ•°
```

---

## 3. PatchGANåˆ¤åˆ«å™¨å®Œæ•´å®žçŽ°

### 3.1 NLayerDiscriminator ç±»

è¿™æ˜¯REPA-Eä¸­ä½¿ç”¨çš„PatchGANåˆ¤åˆ«å™¨å®žçŽ°ï¼Œä½äºŽ `loss/discriminator.py`ã€‚è¯¥å®žçŽ°æ¥è‡ªäºŽTaming Transformersé¡¹ç›®ï¼Œç”¨äºŽæä¾›å¯¹æŠ—è®­ç»ƒã€‚

```python
# ===== æ–‡ä»¶: loss/discriminator.py =====

import functools
import torch
import torch.nn as nn

class NLayerDiscriminator(nn.Module):
    """å®šä¹‰PatchGANåˆ¤åˆ«å™¨ (ç±»ä¼¼äºŽPix2Pix)
    
    æ ¸å¿ƒç‰¹ç‚¹ï¼š
    1. å¤šå±‚å·ç§¯æž¶æž„ï¼Œé€æ¸å¢žåŠ é€šé“æ•°
    2. æ”¯æŒActNormå’ŒBatchNormä¸¤ç§å½’ä¸€åŒ–æ–¹å¼
    3. è¾“å‡ºå•é€šé“é¢„æµ‹å›¾ï¼Œç”¨äºŽpatch-levelçš„çœŸå‡åˆ¤æ–­
    """
    
    def __init__(self, input_nc=3, ndf=64, n_layers=3, use_actnorm=False):
        """æž„å»ºPatchGANåˆ¤åˆ«å™¨
        
        Args:
            input_nc (int): è¾“å…¥å›¾åƒçš„é€šé“æ•° (é»˜è®¤3é€šé“RGB)
            ndf (int): æœ€åŽä¸€ä¸ªå·ç§¯å±‚çš„æ»¤æ³¢å™¨æ•°é‡ (é»˜è®¤64)
            n_layers (int): åˆ¤åˆ«å™¨ä¸­çš„å·ç§¯å±‚æ•° (é»˜è®¤3å±‚)
            use_actnorm (bool): æ˜¯å¦ä½¿ç”¨ActNormå½’ä¸€åŒ– (é»˜è®¤Falseï¼Œä½¿ç”¨BatchNorm)
        """
        super(NLayerDiscriminator, self).__init__()
        
        # === é€‰æ‹©å½’ä¸€åŒ–å±‚ç±»åž‹ ===
        if not use_actnorm:
            norm_layer = nn.BatchNorm2d  # REPA-Eä¸­é»˜è®¤ä½¿ç”¨BatchNorm
        else:
            norm_layer = ActNorm
            
        # æ ¹æ®å½’ä¸€åŒ–å±‚å†³å®šæ˜¯å¦ä½¿ç”¨bias
        if type(norm_layer) == functools.partial:
            use_bias = norm_layer.func != nn.BatchNorm2d
        else:
            use_bias = norm_layer != nn.BatchNorm2d

        # === ç½‘ç»œæž¶æž„æž„å»º ===
        kw = 4  # å·ç§¯æ ¸å¤§å°
        padw = 1  # å¡«å……å¤§å°
        
        # ç¬¬ä¸€å±‚ï¼šè¾“å…¥å±‚ -> ndfé€šé“
        sequence = [
            nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), 
            nn.LeakyReLU(0.2, True)
        ]
        
        # ä¸­é—´å±‚ï¼šé€æ¸å¢žåŠ é€šé“æ•°
        nf_mult = 1
        nf_mult_prev = 1
        for n in range(1, n_layers):
            nf_mult_prev = nf_mult
            nf_mult = min(2 ** n, 8)  # é€šé“æ•°æœ€å¤šå¢žåŠ åˆ°8å€
            sequence += [
                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, 
                         kernel_size=kw, stride=2, padding=padw, bias=use_bias),
                norm_layer(ndf * nf_mult),
                nn.LeakyReLU(0.2, True)
            ]

        # æœ€åŽç¬¬äºŒå±‚ï¼šstride=1çš„å·ç§¯
        nf_mult_prev = nf_mult
        nf_mult = min(2 ** n_layers, 8)
        sequence += [
            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, 
                     kernel_size=kw, stride=1, padding=padw, bias=use_bias),
            norm_layer(ndf * nf_mult),
            nn.LeakyReLU(0.2, True)
        ]

        # è¾“å‡ºå±‚ï¼šè¾“å‡ºå•é€šé“é¢„æµ‹å›¾
        sequence += [
            nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)
        ]
        
        self.main = nn.Sequential(*sequence)

    def forward(self, input):
        """å‰å‘ä¼ æ’­
        
        Args:
            input: è¾“å…¥å›¾åƒ [B, C, H, W]
        Returns:
            è¾“å‡ºåˆ¤åˆ«ç»“æžœ [B, 1, H', W'] - patchçº§åˆ«çš„çœŸå‡é¢„æµ‹
        """
        return self.main(input)
```

### 3.2 ActNorm å½’ä¸€åŒ–å±‚

```python
class ActNorm(nn.Module):
    """æ¿€æ´»å½’ä¸€åŒ–å±‚ - å¯æ›¿ä»£BatchNormçš„å½’ä¸€åŒ–æ–¹å¼
    
    ç‰¹ç‚¹ï¼š
    1. æ•°æ®ç›¸å…³çš„åˆå§‹åŒ–ï¼šç¬¬ä¸€æ¬¡å‰å‘ä¼ æ’­æ—¶æ ¹æ®æ•°æ®ç»Ÿè®¡åˆå§‹åŒ–
    2. ä»¿å°„å˜æ¢ï¼šæ”¯æŒscaleå’Œshiftå‚æ•°
    3. å¯é€†æ“ä½œï¼šæ”¯æŒé€†å˜æ¢
    """
    
    def __init__(self, num_features, logdet=False, affine=True, allow_reverse_init=False):
        """åˆå§‹åŒ–ActNormå±‚
        
        Args:
            num_features: ç‰¹å¾é€šé“æ•°
            logdet: æ˜¯å¦è®¡ç®—log determinant (ç”¨äºŽæµæ¨¡åž‹)
            affine: æ˜¯å¦ä½¿ç”¨ä»¿å°„å˜æ¢
            allow_reverse_init: æ˜¯å¦å…è®¸åå‘åˆå§‹åŒ–
        """
        assert affine  # REPA-Eä¸­å¿…é¡»ä½¿ç”¨ä»¿å°„å˜æ¢
        super().__init__()
        self.logdet = logdet
        
        # å¯å­¦ä¹ å‚æ•°ï¼šä½ç½®å’Œå°ºåº¦
        self.loc = nn.Parameter(torch.zeros(1, num_features, 1, 1))    # shiftå‚æ•°
        self.scale = nn.Parameter(torch.ones(1, num_features, 1, 1))   # scaleå‚æ•°
        self.allow_reverse_init = allow_reverse_init

        # åˆå§‹åŒ–æ ‡è®°
        self.register_buffer('initialized', torch.tensor(0, dtype=torch.uint8))

    def initialize(self, input):
        """æ•°æ®ç›¸å…³çš„åˆå§‹åŒ– - æ ¹æ®è¾“å…¥æ•°æ®çš„ç»Ÿè®¡é‡åˆå§‹åŒ–å‚æ•°"""
        with torch.no_grad():
            # è®¡ç®—æ¯ä¸ªé€šé“çš„ç»Ÿè®¡é‡
            flatten = input.permute(1, 0, 2, 3).contiguous().view(input.shape[1], -1)
            mean = (
                flatten.mean(1)
                .unsqueeze(1)
                .unsqueeze(2)
                .unsqueeze(3)
                .permute(1, 0, 2, 3)
            )
            std = (
                flatten.std(1)
                .unsqueeze(1)
                .unsqueeze(2)
                .unsqueeze(3)
                .permute(1, 0, 2, 3)
            )

            # è®¾ç½®å‚æ•°ä½¿å¾—è¾“å‡ºä¸ºæ ‡å‡†æ­£æ€åˆ†å¸ƒ
            self.loc.data.copy_(-mean)
            self.scale.data.copy_(1 / (std + 1e-6))

    def forward(self, input, reverse=False):
        """å‰å‘ä¼ æ’­
        
        Args:
            input: è¾“å…¥å¼ é‡
            reverse: æ˜¯å¦æ‰§è¡Œé€†å˜æ¢
        Returns:
            å½’ä¸€åŒ–åŽçš„å¼ é‡
        """
        if reverse:
            return self.reverse(input)
            
        # å¤„ç†2Dè¾“å…¥
        if len(input.shape) == 2:
            input = input[:, :, None, None]
            squeeze = True
        else:
            squeeze = False

        _, _, height, width = input.shape

        # é¦–æ¬¡å‰å‘ä¼ æ’­æ—¶åˆå§‹åŒ–
        if self.training and self.initialized.item() == 0:
            self.initialize(input)
            self.initialized.fill_(1)

        # ä»¿å°„å˜æ¢: scale * (input + shift)
        h = self.scale * (input + self.loc)

        if squeeze:
            h = h.squeeze(-1).squeeze(-1)

        # å¯é€‰ï¼šè®¡ç®—log determinant
        if self.logdet:
            log_abs = torch.log(torch.abs(self.scale))
            logdet = height * width * torch.sum(log_abs)
            logdet = logdet * torch.ones(input.shape[0]).to(input)
            return h, logdet

        return h

    def reverse(self, output):
        """é€†å˜æ¢ï¼šä»Žå½’ä¸€åŒ–åŽçš„è¾“å‡ºæ¢å¤åŽŸå§‹è¾“å…¥"""
        if self.training and self.initialized.item() == 0:
            if not self.allow_reverse_init:
                raise RuntimeError(
                    "Initializing ActNorm in reverse direction is "
                    "disabled by default. Use allow_reverse_init=True to enable."
                )
            else:
                self.initialize(output)
                self.initialized.fill_(1)

        if len(output.shape) == 2:
            output = output[:, :, None, None]
            squeeze = True
        else:
            squeeze = False

        # é€†ä»¿å°„å˜æ¢: output / scale - shift
        h = output / self.scale - self.loc

        if squeeze:
            h = h.squeeze(-1).squeeze(-1)
        return h
```

### 3.3 æƒé‡åˆå§‹åŒ–å‡½æ•°

```python
def weights_init(m):
    """æƒé‡åˆå§‹åŒ–å‡½æ•° - ç”¨äºŽåˆ¤åˆ«å™¨å‚æ•°åˆå§‹åŒ–
    
    åˆå§‹åŒ–ç­–ç•¥ï¼š
    1. å·ç§¯å±‚ï¼šæ­£æ€åˆ†å¸ƒåˆå§‹åŒ– (mean=0.0, std=0.02)
    2. BatchNormå±‚ï¼šæƒé‡æ­£æ€åˆ†å¸ƒ (mean=1.0, std=0.02)ï¼Œåç½®ä¸º0
    
    Args:
        m: ç½‘ç»œæ¨¡å—
    """
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        # å·ç§¯å±‚æƒé‡åˆå§‹åŒ–
        nn.init.normal_(m.weight.data, 0.0, 0.02)
    elif classname.find('BatchNorm') != -1:
        # BatchNormå±‚æƒé‡å’Œåç½®åˆå§‹åŒ–
        nn.init.normal_(m.weight.data, 1.0, 0.02)
        nn.init.constant_(m.bias.data, 0)
```

### 3.4 åˆ¤åˆ«å™¨åœ¨REPA-Eä¸­çš„ä½¿ç”¨

```python
# ===== åœ¨ReconstructionLoss_Single_Stageä¸­çš„ä½¿ç”¨ç¤ºä¾‹ =====

class ReconstructionLoss_Single_Stage:
    def __init__(self, config):
        # åˆ›å»ºåˆ¤åˆ«å™¨å¹¶åº”ç”¨æƒé‡åˆå§‹åŒ–
        self.discriminator = NLayerDiscriminator(
            input_nc=3,        # RGBå›¾åƒ
            n_layers=3,        # 3å±‚å·ç§¯
            use_actnorm=False  # ä½¿ç”¨BatchNormè€Œä¸æ˜¯ActNorm
        ).apply(weights_init)  # åº”ç”¨æƒé‡åˆå§‹åŒ–
        
        # åˆ¤åˆ«å™¨è®­ç»ƒå‚æ•°è®¾ç½®
        self.discriminator_iter_start = loss_config.discriminator_start     # å¼€å§‹è®­ç»ƒåˆ¤åˆ«å™¨çš„æ­¥æ•°
        self.discriminator_factor = loss_config.discriminator_factor       # åˆ¤åˆ«å™¨æŸå¤±ç³»æ•°
        self.discriminator_weight = loss_config.discriminator_weight       # åˆ¤åˆ«å™¨æƒé‡

    def _forward_discriminator(self, inputs, reconstructions, global_step):
        """åˆ¤åˆ«å™¨è®­ç»ƒæ­¥éª¤"""
        # å¯ç”¨åˆ¤åˆ«å™¨æ¢¯åº¦
        for param in self.discriminator.parameters():
            param.requires_grad = True
        
        # åˆ¤åˆ«çœŸå®žå’Œé‡å»ºå›¾åƒ
        logits_real = self.discriminator(inputs.detach())
        logits_fake = self.discriminator(reconstructions.detach())
        
        # è®¡ç®—HingeæŸå¤±
        discriminator_loss = hinge_d_loss(logits_real, logits_fake)
        
        return discriminator_loss
    
    def _forward_generator(self, inputs, reconstructions):
        """ç”Ÿæˆå™¨è®­ç»ƒæ—¶ä½¿ç”¨åˆ¤åˆ«å™¨"""
        # ç¦ç”¨åˆ¤åˆ«å™¨æ¢¯åº¦ï¼ˆåªæ›´æ–°ç”Ÿæˆå™¨ï¼‰
        for param in self.discriminator.parameters():
            param.requires_grad = False
            
        # è®¡ç®—ç”Ÿæˆå™¨å¯¹æŠ—æŸå¤±
        logits_fake = self.discriminator(reconstructions)
        generator_loss = -torch.mean(logits_fake)  # æœ€å¤§åŒ–åˆ¤åˆ«å™¨å¯¹ç”Ÿæˆå›¾åƒçš„ç½®ä¿¡åº¦
        
        return generator_loss
```

---

## 4. SiTæ¨¡åž‹ä¸­çš„æŠ•å½±å¯¹é½é›†æˆ

### 4.1 MLPæŠ•å½±å™¨æž„å»º

ä½äºŽ `models/sit.py:23-30` çš„æ ¸å¿ƒæŠ•å½±å™¨æž„å»ºå‡½æ•°ï¼š

```python
# ===== æ–‡ä»¶: models/sit.py =====

def build_mlp(hidden_size, projector_dim, z_dim):
    """æž„å»ºMLPæŠ•å½±å™¨ - REPA-Eçš„æ ¸å¿ƒç»„ä»¶
    
    æž¶æž„ï¼šä¸‰å±‚MLPï¼Œä¸¤ä¸ªéšè—å±‚ä½¿ç”¨SiLUæ¿€æ´»
    
    Args:
        hidden_size: SiTæ¨¡åž‹çš„éšè—å±‚ç»´åº¦ (ä¾‹å¦‚: 1152 for XL)
        projector_dim: æŠ•å½±å™¨ä¸­é—´å±‚ç»´åº¦ (é»˜è®¤: 2048)
        z_dim: ç›®æ ‡ç‰¹å¾ç»´åº¦ï¼Œå¯¹åº”è§†è§‰ç¼–ç å™¨çš„ç‰¹å¾ç»´åº¦ (ä¾‹å¦‚: 768 for DINOv2-B)
    
    Returns:
        nn.Sequential: ä¸‰å±‚MLPæŠ•å½±å™¨
    """
    return nn.Sequential(
        nn.Linear(hidden_size, projector_dim),   # ç¬¬ä¸€å±‚ï¼šæ‰©å±•åˆ°ä¸­é—´ç»´åº¦
        nn.SiLU(),                              # SiLUæ¿€æ´»å‡½æ•°
        nn.Linear(projector_dim, projector_dim), # ç¬¬äºŒå±‚ï¼šä¿æŒä¸­é—´ç»´åº¦
        nn.SiLU(),                              # SiLUæ¿€æ´»å‡½æ•°  
        nn.Linear(projector_dim, z_dim),        # ç¬¬ä¸‰å±‚ï¼šæ˜ å°„åˆ°ç›®æ ‡ç»´åº¦
    )

def mean_flat(x):
    """åœ¨é™¤batchç»´åº¦å¤–çš„æ‰€æœ‰ç»´åº¦ä¸Šè®¡ç®—å¹³å‡å€¼ - ç”¨äºŽæŸå¤±è®¡ç®—
    
    Args:
        x: è¾“å…¥å¼ é‡ï¼Œä»»æ„å½¢çŠ¶
    Returns:
        åœ¨ç©ºé—´ç»´åº¦ä¸Šå¹³å‡åŽçš„å¼ é‡ [B,]
    """
    return torch.mean(x, dim=list(range(1, len(x.size()))))
```

### 4.2 SiTæ¨¡åž‹ä¸­çš„BatchNormå±‚å®žçŽ°

```python
class SiT(nn.Module):
    """Scalable Interpolant Transformer - é›†æˆæŠ•å½±å¯¹é½çš„æ‰©æ•£æ¨¡åž‹
    
    æ ¸å¿ƒç‰¹ç‚¹ï¼š
    1. é›†æˆMLPæŠ•å½±å™¨å®žçŽ°ç‰¹å¾å¯¹é½
    2. BatchNormå±‚å¤„ç†VAEç‰¹å¾å½’ä¸€åŒ–
    3. å‰å‘ä¼ æ’­ä¸­è®¡ç®—æŠ•å½±å¯¹é½æŸå¤±
    """
    
    def __init__(
        self,
        input_size=32,
        patch_size=2,
        in_channels=4,
        hidden_size=1152,
        depth=28,
        num_heads=16,
        mlp_ratio=4.0,
        class_dropout_prob=0.1,
        num_classes=1000,
        z_dims=[768],           # è§†è§‰ç¼–ç å™¨ç‰¹å¾ç»´åº¦åˆ—è¡¨ (æ”¯æŒå¤šä¸ªç¼–ç å™¨)
        projector_dim=2048,     # MLPæŠ•å½±å™¨ä¸­é—´å±‚ç»´åº¦
        bn_momentum=0.1,        # BatchNormåŠ¨é‡å‚æ•° ðŸ”¥æ ¸å¿ƒå‚æ•°!
        **block_kwargs
    ):
        super().__init__()
        
        # === æ ¸å¿ƒç»„ä»¶åˆå§‹åŒ– ===
        self.path_type = path_type
        self.in_channels = in_channels
        self.out_channels = in_channels
        self.patch_size = patch_size
        self.num_heads = num_heads
        self.encoder_depth = encoder_depth  # æå–ç‰¹å¾çš„å±‚æ·±åº¦

        # === MLPæŠ•å½±å™¨åˆ—è¡¨ ===
        # ä¸ºæ¯ä¸ªè§†è§‰ç¼–ç å™¨åˆ›å»ºå¯¹åº”çš„æŠ•å½±å™¨
        self.projectors = nn.ModuleList([
            build_mlp(hidden_size, projector_dim, z_dim) for z_dim in z_dims
        ])
        
        # === æ ¸å¿ƒåˆ›æ–°ï¼šBatchNormå±‚ ===
        # ç”¨äºŽå½’ä¸€åŒ–VAEè¾“å‡ºï¼Œè§£å†³ç«¯åˆ°ç«¯è®­ç»ƒä¸­çš„ç‰¹å¾ç»Ÿè®¡é—®é¢˜
        self.bn = torch.nn.BatchNorm2d(
            in_channels,              # è¾“å…¥é€šé“æ•° (VAEè¾“å‡ºé€šé“)
            eps=1e-4,                # æ•°å€¼ç¨³å®šæ€§å‚æ•°
            momentum=bn_momentum,     # ðŸ”¥å…³é”®ï¼šåŠ¨é‡å‚æ•°ï¼ŒæŽ§åˆ¶ç»Ÿè®¡é‡æ›´æ–°é€Ÿåº¦
            affine=False,            # ðŸ”¥å…³é”®ï¼šç¦ç”¨ä»¿å°„å˜æ¢ï¼Œé¿å…å‚æ•°hackæ‰©æ•£æŸå¤±
            track_running_stats=True  # è·Ÿè¸ªè¿è¡Œæ—¶ç»Ÿè®¡é‡
        )
        self.bn.reset_running_stats()  # é‡ç½®ç»Ÿè®¡é‡
        
        # === å…¶ä»–æ ‡å‡†ç»„ä»¶ ===
        self.x_embedder = PatchEmbed(input_size, patch_size, in_channels, hidden_size, bias=True)
        self.t_embedder = TimestepEmbedder(hidden_size)
        self.y_embedder = LabelEmbedder(num_classes, hidden_size, class_dropout_prob)
        # ... å…¶ä»–ç»„ä»¶åˆå§‹åŒ–

    def forward(self, x, y, zs, loss_kwargs, time_input=None, noises=None):
        """SiTå‰å‘ä¼ æ’­ - é›†æˆæŸå¤±å‡½æ•°è®¡ç®—
        
        Args:
            x: è¾“å…¥å›¾åƒ/æ½œåœ¨è¡¨ç¤º [N, C, H, W] - æœªå½’ä¸€åŒ–çš„VAEè¾“å‡º
            y: ç±»åˆ«æ ‡ç­¾ [N,]
            zs: å¤–éƒ¨è§†è§‰ç‰¹å¾åˆ—è¡¨ [N, L, C'] - æ¥è‡ªè§†è§‰ç¼–ç å™¨
            loss_kwargs: æŸå¤±å‡½æ•°å‚æ•°å­—å…¸
            time_input: å¯é€‰çš„æ—¶é—´æ­¥å¼ é‡
            noises: å¯é€‰çš„å™ªå£°å¼ é‡
            
        Returns:
            åŒ…å«å¯¹é½ç‰¹å¾ã€åŽ»å™ªæŸå¤±ã€æŠ•å½±æŸå¤±ç­‰çš„å­—å…¸
        """
        # === ç¬¬1æ­¥ï¼šBatchNormå½’ä¸€åŒ– ===
        # ðŸ”¥æ ¸å¿ƒåˆ›æ–°ï¼šä½¿ç”¨BatchNormå½’ä¸€åŒ–VAEè¾“å‡º
        normalized_x = self.bn(x)  # åŠ¨æ€å½’ä¸€åŒ–ï¼Œæ— éœ€é‡æ–°è®¡ç®—æ•°æ®é›†ç»Ÿè®¡é‡
        
        # === ç¬¬2æ­¥ï¼šé‡‡æ ·æ—¶é—´æ­¥ï¼ˆå¦‚æžœæœªæä¾›ï¼‰ ===
        if time_input is None:
            if loss_kwargs["weighting"] == "uniform":
                time_input = torch.rand((normalized_x.shape[0], 1, 1, 1))
            elif loss_kwargs["weighting"] == "lognormal":
                # EDMé£Žæ ¼çš„å¯¹æ•°æ­£æ€åˆ†å¸ƒé‡‡æ ·
                rnd_normal = torch.randn((normalized_x.shape[0], 1, 1, 1))
                sigma = rnd_normal.exp()
                if loss_kwargs["path_type"] == "linear":
                    time_input = sigma / (1 + sigma)
                elif loss_kwargs["path_type"] == "cosine":
                    time_input = 2 / np.pi * torch.atan(sigma)
        time_input = time_input.to(device=normalized_x.device, dtype=normalized_x.dtype)

        # === ç¬¬3æ­¥ï¼šé‡‡æ ·å™ªå£°ï¼ˆå¦‚æžœæœªæä¾›ï¼‰ ===
        if noises is None:
            noises = torch.randn_like(normalized_x)
        else:
            noises = noises.to(device=normalized_x.device, dtype=normalized_x.dtype)

        # === ç¬¬4æ­¥ï¼šè®¡ç®—æ’å€¼è·¯å¾„ ===
        alpha_t, sigma_t, d_alpha_t, d_sigma_t = self.interpolant(
            time_input, 
            path_type=loss_kwargs["path_type"]
        )
        
        # æž„é€ æ¨¡åž‹è¾“å…¥å’Œç›®æ ‡
        model_input = alpha_t * normalized_x + sigma_t * noises
        if loss_kwargs["prediction"] == 'v':
            model_target = d_alpha_t * normalized_x + d_sigma_t * noises
        
        # === ç¬¬5æ­¥ï¼šTransformerå‰å‘ä¼ æ’­ ===
        x = self.x_embedder(model_input) + self.pos_embed  # [N, T, D]
        N, T, D = x.shape
        
        # æ—¶é—´æ­¥å’Œç±»åˆ«åµŒå…¥
        t_embed = self.t_embedder(time_input.flatten())
        y = self.y_embedder(y, self.training)
        c = t_embed + y  # æ¡ä»¶åµŒå…¥

        # === ç¬¬6æ­¥ï¼šæ ¸å¿ƒç‰¹å¾æå–å’ŒæŠ•å½±å¯¹é½ ===
        for i, block in enumerate(self.blocks):
            x = block(x, c)  # Transformer block
            
            # ðŸ”¥å…³é”®ï¼šåœ¨æŒ‡å®šæ·±åº¦æå–ç‰¹å¾å¹¶æŠ•å½±
            if (i + 1) == self.encoder_depth:
                # ä½¿ç”¨å¤šä¸ªæŠ•å½±å™¨å¤„ç†ç‰¹å¾
                zs_tilde = [
                    projector(x.reshape(-1, D)).reshape(N, T, -1) 
                    for projector in self.projectors
                ]
                
                # ä»…å¯¹é½æ¨¡å¼ï¼šè·³è¿‡åŽç»­è®¡ç®—
                if loss_kwargs["align_only"]:
                    break
        
        # === ç¬¬7æ­¥ï¼šæœ€ç»ˆè¾“å‡ºç”Ÿæˆ ===
        if not loss_kwargs["align_only"]:
            x = self.final_layer(x, c)  # [N, T, patch_size^2 * out_channels]
            x = self.unpatchify(x)      # [N, out_channels, H, W]

        # === ç¬¬8æ­¥ï¼šæŸå¤±è®¡ç®— ===
        # åŽ»å™ªæŸå¤±
        denoising_loss = None if loss_kwargs["align_only"] else mean_flat((x - model_target) ** 2)

        # ðŸ”¥æ ¸å¿ƒåˆ›æ–°ï¼šæŠ•å½±å¯¹é½æŸå¤±è®¡ç®—
        proj_loss = torch.tensor(0., device=x.device)
        bsz = zs[0].shape[0]
        
        for i, (z, z_tilde) in enumerate(zip(zs, zs_tilde)):
            for z_j, z_tilde_j in zip(z, z_tilde):
                # L2å½’ä¸€åŒ–
                z_tilde_j = torch.nn.functional.normalize(z_tilde_j, dim=-1)  # æŠ•å½±ç‰¹å¾
                z_j = torch.nn.functional.normalize(z_j, dim=-1)            # è§†è§‰ç¼–ç å™¨ç‰¹å¾
                
                # è´Ÿä½™å¼¦ç›¸ä¼¼åº¦æŸå¤± (é¼“åŠ±ç‰¹å¾å¯¹é½)
                proj_loss += mean_flat(-(z_j * z_tilde_j).sum(dim=-1))
        
        proj_loss /= (len(zs) * bsz)  # å½’ä¸€åŒ–

        return {
            "zs_tilde": zs_tilde,       # æŠ•å½±åŽçš„SiTç‰¹å¾
            "model_output": x,          # æ¨¡åž‹è¾“å‡º
            "denoising_loss": denoising_loss,  # åŽ»å™ªæŸå¤±
            "proj_loss": proj_loss,     # ðŸ”¥æŠ•å½±å¯¹é½æŸå¤±
            "time_input": time_input,   # æ—¶é—´æ­¥
            "noises": noises,          # å™ªå£°
        }
```

### 4.3 æ’å€¼è·¯å¾„è®¡ç®—

```python
def interpolant(self, t, path_type=None):
    """è®¡ç®—æ’å€¼è·¯å¾„ç³»æ•° - æ”¯æŒçº¿æ€§å’Œä½™å¼¦è·¯å¾„
    
    Args:
        t: æ—¶é—´æ­¥ [B, 1, 1, 1]
        path_type: è·¯å¾„ç±»åž‹ ("linear" or "cosine")
        
    Returns:
        alpha_t, sigma_t, d_alpha_t, d_sigma_t: æ’å€¼ç³»æ•°
    """
    if path_type == "linear":
        alpha_t = 1 - t      # æ•°æ®ç³»æ•°
        sigma_t = t          # å™ªå£°ç³»æ•°
        d_alpha_t = -1       # alphaçš„å¯¼æ•°
        d_sigma_t = 1        # sigmaçš„å¯¼æ•°
    elif path_type == "cosine":
        alpha_t = torch.cos(t * np.pi / 2)
        sigma_t = torch.sin(t * np.pi / 2)
        d_alpha_t = -np.pi / 2 * torch.sin(t * np.pi / 2)
        d_sigma_t = np.pi / 2 * torch.cos(t * np.pi / 2)
    else:
        raise NotImplementedError()

    return alpha_t, sigma_t, d_alpha_t, d_sigma_t
```

### 4.4 å…³é”®å‚æ•°è¯´æ˜Ž

```python
# === SiTæ¨¡åž‹å…³é”®å‚æ•°é…ç½® ===

# æŠ•å½±å¯¹é½ç›¸å…³å‚æ•°
z_dims = [768]          # è§†è§‰ç¼–ç å™¨ç‰¹å¾ç»´åº¦ (DINOv2-B: 768, DINOv2-L: 1024)
projector_dim = 2048    # MLPæŠ•å½±å™¨ä¸­é—´å±‚ç»´åº¦
encoder_depth = 8       # ç‰¹å¾æå–æ·±åº¦ (ä»Žç¬¬8å±‚æå–ç‰¹å¾è¿›è¡Œå¯¹é½)

# BatchNormå‚æ•°  
bn_momentum = 0.1       # ðŸ”¥å…³é”®å‚æ•°ï¼šBatchNormåŠ¨é‡ï¼ŒæŽ§åˆ¶ç»Ÿè®¡é‡æ›´æ–°é€Ÿåº¦
                        # è¾ƒå°å€¼=æ›´ç¨³å®šä½†é€‚åº”æ…¢ï¼Œè¾ƒå¤§å€¼=å¿«é€Ÿé€‚åº”ä½†å¯èƒ½ä¸ç¨³å®š

# è®­ç»ƒå‚æ•°
proj_coeff = 0.5        # SiTæŠ•å½±å¯¹é½æŸå¤±ç³»æ•° (åœ¨train_repae.pyä¸­è®¾ç½®)

# BatchNormè®¾ç½®è¯´æ˜Ž
affine = False          # ðŸ”¥ç¦ç”¨ä»¿å°„å˜æ¢ï¼Œé¿å…å‚æ•°hackæ‰©æ•£æŸå¤±
track_running_stats = True  # è·Ÿè¸ªè¿è¡Œç»Ÿè®¡é‡ï¼Œç”¨äºŽæŽ¨ç†æ—¶å½’ä¸€åŒ–
```

### 4.5 ä¸ŽVAEç«¯å¯¹é½æŸå¤±çš„åŒºåˆ«

```python
# === SiTç«¯æŠ•å½±å¯¹é½æŸå¤± (models/sit.py:372-379) ===
proj_loss = torch.tensor(0., device=x.device)
for i, (z, z_tilde) in enumerate(zip(zs, zs_tilde)):
    for z_j, z_tilde_j in zip(z, z_tilde):
        z_tilde_j = torch.nn.functional.normalize(z_tilde_j, dim=-1)  # SiTæŠ•å½±ç‰¹å¾
        z_j = torch.nn.functional.normalize(z_j, dim=-1)            # è§†è§‰ç¼–ç å™¨ç‰¹å¾
        proj_loss += mean_flat(-(z_j * z_tilde_j).sum(dim=-1))

# === VAEç«¯æŠ•å½±å¯¹é½æŸå¤± (loss/losses.py:411-422) ===
proj_loss = torch.tensor(0., device=inputs.device)  
for i, (z, z_tilde) in enumerate(zip(zs, zs_tilde)):
    for j, (z_j, z_tilde_j) in enumerate(zip(z, z_tilde)):
        z_tilde_j = torch.nn.functional.normalize(z_tilde_j, dim=-1)  # VAEæŠ•å½±ç‰¹å¾
        z_j = torch.nn.functional.normalize(z_j, dim=-1)             # è§†è§‰ç¼–ç å™¨ç‰¹å¾
        proj_loss += mean_flat(-(z_j * z_tilde_j).sum(dim=-1))

# å…±åŒç‰¹ç‚¹ï¼šéƒ½ä½¿ç”¨è´Ÿä½™å¼¦ç›¸ä¼¼åº¦ä½œä¸ºå¯¹é½ç›®æ ‡
# åŒºåˆ«ï¼šSiTç«¯åœ¨Transformerä¸­é—´å±‚æå–ç‰¹å¾ï¼ŒVAEç«¯åœ¨é‡å»ºè¿‡ç¨‹ä¸­å¯¹é½
```

---

## 5. ç«¯åˆ°ç«¯è®­ç»ƒå¾ªçŽ¯å®žçŽ°

### 5.1 ä¸‰ä¼˜åŒ–å™¨æž¶æž„

REPA-Eçš„æ ¸å¿ƒåˆ›æ–°ä¹‹ä¸€æ˜¯ä½¿ç”¨ä¸‰ä¸ªç‹¬ç«‹çš„ä¼˜åŒ–å™¨ï¼Œåˆ†åˆ«ä¼˜åŒ–ä¸åŒçš„ç»„ä»¶ï¼š

```python
# ===== æ–‡ä»¶: train_repae.py:225-246 =====

# === ä¼˜åŒ–å™¨1: SiTæ¨¡åž‹ä¼˜åŒ–å™¨ ===
optimizer = torch.optim.AdamW(
    model.parameters(),                    # SiTæ¨¡åž‹å‚æ•°
    lr=args.learning_rate,                # å­¦ä¹ çŽ‡ (é»˜è®¤: 1e-4)
    betas=(args.adam_beta1, args.adam_beta2),  # Adam betaå‚æ•°
    weight_decay=args.adam_weight_decay,   # æƒé‡è¡°å‡
    eps=args.adam_epsilon,                 # æ•°å€¼ç¨³å®šæ€§å‚æ•°
)

# === ä¼˜åŒ–å™¨2: VAEä¼˜åŒ–å™¨ ===
optimizer_vae = torch.optim.AdamW(
    vae.parameters(),                      # VAEå‚æ•°
    lr=args.vae_learning_rate,            # VAEå­¦ä¹ çŽ‡ (é»˜è®¤: 1e-4)
    betas=(args.adam_beta1, args.adam_beta2),
    weight_decay=args.adam_weight_decay,
    eps=args.adam_epsilon,
)

# === ä¼˜åŒ–å™¨3: åˆ¤åˆ«å™¨ä¼˜åŒ–å™¨ ===
optimizer_loss_fn = torch.optim.AdamW(
    vae_loss_fn.parameters(),             # åˆ¤åˆ«å™¨å‚æ•° (åœ¨æŸå¤±å‡½æ•°ä¸­)
    lr=args.disc_learning_rate,           # åˆ¤åˆ«å™¨å­¦ä¹ çŽ‡ (é»˜è®¤: 1e-4)  
    betas=(args.adam_beta1, args.adam_beta2),
    weight_decay=args.adam_weight_decay,
    eps=args.adam_epsilon,
)
```

### 5.2 å…³é”®è¾…åŠ©å‡½æ•°

```python
# ===== æ–‡ä»¶: train_repae.py =====

def requires_grad(model, flag=True):
    """è®¾ç½®æ¨¡åž‹æ‰€æœ‰å‚æ•°çš„requires_gradæ ‡å¿—
    
    ç”¨é€”ï¼šæŽ§åˆ¶å“ªäº›æ¨¡åž‹å‚ä¸Žæ¢¯åº¦è®¡ç®—ï¼Œå®žçŽ°Stop-gradientæœºåˆ¶
    
    Args:
        model: è¦è®¾ç½®çš„æ¨¡åž‹
        flag: Trueå¯ç”¨æ¢¯åº¦ï¼ŒFalseç¦ç”¨æ¢¯åº¦
    """
    for p in model.parameters():
        p.requires_grad = flag

@torch.no_grad()
def update_ema(ema_model, model, decay=0.9999):
    """æ›´æ–°EMAæ¨¡åž‹ - æŒ‡æ•°ç§»åŠ¨å¹³å‡
    
    Args:
        ema_model: EMAæ¨¡åž‹
        model: å½“å‰è®­ç»ƒçš„æ¨¡åž‹
        decay: EMAè¡°å‡ç³»æ•° (é»˜è®¤: 0.9999)
    """
    ema_params = OrderedDict(ema_model.named_parameters())
    model_params = OrderedDict(model.named_parameters())
    
    for name, param in model_params.items():
        name = name.replace("module.", "")
        # EMAæ›´æ–°å…¬å¼: ema = decay * ema + (1 - decay) * current
        ema_params[name].mul_(decay).add_(param.data, alpha=1 - decay)
    
    # åŒæ—¶å¯¹BNç¼“å†²åŒºæ‰§è¡ŒEMA
    ema_buffers = OrderedDict(ema_model.named_buffers())
    model_buffers = OrderedDict(model.named_buffers())
    for name, buffer in model_buffers.items():
        name = name.replace("module.", "")
        if buffer.dtype in (torch.bfloat16, torch.float16, torch.float32, torch.float64):
            # ä»…å¯¹æµ®ç‚¹ç¼“å†²åŒºåº”ç”¨EMA
            ema_buffers[name].mul_(decay).add_(buffer.data, alpha=1 - decay)
        else:
            # éžæµ®ç‚¹ç¼“å†²åŒºç›´æŽ¥å¤åˆ¶
            ema_buffers[name].copy_(buffer.data)
```

### 5.3 ç«¯åˆ°ç«¯è®­ç»ƒå¾ªçŽ¯çš„æ ¸å¿ƒæ­¥éª¤

```python
# ===== å®Œæ•´çš„ç«¯åˆ°ç«¯è®­ç»ƒå¾ªçŽ¯ =====

def training_loop():
    """REPA-Eç«¯åˆ°ç«¯è®­ç»ƒçš„æ ¸å¿ƒå¾ªçŽ¯"""
    
    for epoch in range(num_epochs):
        for batch_idx, (raw_image, labels) in enumerate(train_dataloader):
            
            # === æ­¥éª¤1: æ•°æ®é¢„å¤„ç†å’Œç‰¹å¾æå– ===
            with accelerator.accumulate([model, vae, vae_loss_fn]), accelerator.autocast():
                
                # VAEé¢„å¤„ç†: [0,255] -> [-1,1]
                processed_image = preprocess_imgs_vae(raw_image)
                
                # VAEå‰å‘ä¼ æ’­
                posterior, z, recon_image = vae(processed_image)
                
                # è§†è§‰ç¼–ç å™¨ç‰¹å¾æå–
                zs = encoders(processed_image)  # ä»Žè§†è§‰ç¼–ç å™¨æå–ç‰¹å¾
                
                # === æ­¥éª¤2: VAEè®­ç»ƒ (åŒ…å«å¯¹é½æŸå¤±) ===
                
                # ðŸ”¥å…³é”®ï¼šç¦ç”¨SiTæ¢¯åº¦ï¼Œé¿å…REPAæ¢¯åº¦å½±å“SiT
                requires_grad(model, False)
                model.eval()  # é¿å…BNç»Ÿè®¡é‡è¢«VAEæ›´æ–°
                
                # è®¡ç®—VAEé‡å»ºæŸå¤± (L1 + LPIPS + KL + GAN)
                vae_loss, vae_loss_dict = vae_loss_fn(
                    processed_image, recon_image, posterior, global_step, "generator"
                )
                vae_loss = vae_loss.mean()
                
                # ðŸ”¥æ ¸å¿ƒåˆ›æ–°ï¼šè®¡ç®—VAEçš„REPAå¯¹é½æŸå¤±
                loss_kwargs = dict(
                    path_type=args.path_type,
                    prediction=args.prediction, 
                    weighting=args.weighting,
                    align_only=True  # ä»…è®¡ç®—å¯¹é½æŸå¤±ï¼Œä¸è®¡ç®—åŽ»å™ªæŸå¤±
                )
                
                vae_align_outputs = model(
                    x=z,                    # VAEæ½œåœ¨è¡¨ç¤º
                    y=labels,              # ç±»åˆ«æ ‡ç­¾
                    zs=zs,                 # è§†è§‰ç¼–ç å™¨ç‰¹å¾
                    loss_kwargs=loss_kwargs,
                    time_input=time_input,  # å¯é€‰ï¼šå¤ç”¨æ—¶é—´æ­¥
                    noises=noises,         # å¯é€‰ï¼šå¤ç”¨å™ªå£°
                )
                
                # VAEæ€»æŸå¤± = é‡å»ºæŸå¤± + å¯¹é½æŸå¤±
                vae_loss = vae_loss + args.vae_align_proj_coeff * vae_align_outputs["proj_loss"].mean()
                
                # ä¿å­˜æ—¶é—´æ­¥å’Œå™ªå£°ï¼Œä¾›SiTå¤ç”¨
                time_input = vae_align_outputs["time_input"] 
                noises = vae_align_outputs["noises"]
                
                # VAEåå‘ä¼ æ’­å’Œæ›´æ–°
                accelerator.backward(vae_loss)
                if accelerator.sync_gradients:
                    grad_norm_vae = accelerator.clip_grad_norm_(vae.parameters(), args.max_grad_norm)
                optimizer_vae.step()
                optimizer_vae.zero_grad(set_to_none=True)
                
                # === æ­¥éª¤3: åˆ¤åˆ«å™¨è®­ç»ƒ ===
                
                # è®¡ç®—åˆ¤åˆ«å™¨æŸå¤±
                d_loss, d_loss_dict = vae_loss_fn(
                    processed_image, recon_image, posterior, global_step, "discriminator"
                )
                d_loss = d_loss.mean()
                
                # åˆ¤åˆ«å™¨åå‘ä¼ æ’­å’Œæ›´æ–°
                accelerator.backward(d_loss)
                if accelerator.sync_gradients:
                    grad_norm_disc = accelerator.clip_grad_norm_(vae_loss_fn.parameters(), args.max_grad_norm)
                optimizer_loss_fn.step()
                optimizer_loss_fn.zero_grad(set_to_none=True)
                
                # === æ­¥éª¤4: SiTè®­ç»ƒ (åŒ…å«åŽ»å™ªå’Œå¯¹é½æŸå¤±) ===
                
                # ðŸ”¥é‡æ–°å¯ç”¨SiTæ¢¯åº¦
                requires_grad(model, True)
                model.train()
                
                # ðŸ”¥å…³é”®Stop-gradient: åˆ†ç¦»VAEæ½œåœ¨è¡¨ç¤ºï¼Œé¿å…æ‰©æ•£æŸå¤±å½±å“VAE
                loss_kwargs.update({
                    "weighting": args.weighting,
                    "align_only": False  # åŒæ—¶è®¡ç®—åŽ»å™ªæŸå¤±å’Œå¯¹é½æŸå¤±
                })
                
                sit_outputs = model(
                    x=z.detach(),          # ðŸ”¥å…³é”®ï¼šåˆ†ç¦»VAEè¾“å‡ºï¼Œå®žçŽ°stop-gradient
                    y=labels,
                    zs=zs,
                    loss_kwargs=loss_kwargs,
                    time_input=time_input,  # å¤ç”¨VAEé˜¶æ®µçš„æ—¶é—´æ­¥
                    noises=noises,         # å¤ç”¨VAEé˜¶æ®µçš„å™ªå£°
                )
                
                # SiTæ€»æŸå¤± = åŽ»å™ªæŸå¤± + æŠ•å½±å¯¹é½æŸå¤±
                sit_loss = (sit_outputs["denoising_loss"].mean() + 
                           args.proj_coeff * sit_outputs["proj_loss"].mean())
                
                # SiTåå‘ä¼ æ’­å’Œæ›´æ–°
                accelerator.backward(sit_loss)
                if accelerator.sync_gradients:
                    grad_norm_sit = accelerator.clip_grad_norm_(model.parameters(), args.max_grad_norm)
                optimizer.step()
                optimizer.zero_grad(set_to_none=True)
                
                # === æ­¥éª¤5: EMAæ›´æ–° ===
                if accelerator.sync_gradients:
                    unwrapped_model = accelerator.unwrap_model(model)
                    update_ema(ema, unwrapped_model._orig_mod if args.compile else unwrapped_model)
```

### 5.4 æŸå¤±ç³»æ•°å’Œå…³é”®å‚æ•°

```python
# === å…³é”®è®­ç»ƒå‚æ•° ===

# æŸå¤±ç³»æ•°
proj_coeff = 0.5                    # SiTæŠ•å½±å¯¹é½æŸå¤±ç³»æ•°
vae_align_proj_coeff = 1.5          # VAEæŠ•å½±å¯¹é½æŸå¤±ç³»æ•°

# å­¦ä¹ çŽ‡è®¾ç½®
learning_rate = 1e-4                # SiTå­¦ä¹ çŽ‡
vae_learning_rate = 1e-4            # VAEå­¦ä¹ çŽ‡  
disc_learning_rate = 1e-4           # åˆ¤åˆ«å™¨å­¦ä¹ çŽ‡

# EMAå‚æ•°
ema_decay = 0.9999                  # EMAè¡°å‡ç³»æ•°

# æ¢¯åº¦è£å‰ª
max_grad_norm = 1.0                 # æœ€å¤§æ¢¯åº¦èŒƒæ•°

# BatchNormåŠ¨é‡
bn_momentum = 0.1                   # BatchNormåŠ¨é‡å‚æ•°
```

### 5.5 Stop-gradientæœºåˆ¶çš„å…³é”®å®žçŽ°

```python
# === Stop-gradientçš„ä¸‰ä¸ªå…³é”®ç‚¹ ===

# 1. VAEè®­ç»ƒæ—¶ç¦ç”¨SiTæ¢¯åº¦
requires_grad(model, False)    # ç¦ç”¨SiTå‚æ•°æ¢¯åº¦
model.eval()                   # é¿å…BNç»Ÿè®¡é‡è¢«VAEè®­ç»ƒå½±å“

# 2. SiTè®­ç»ƒæ—¶åˆ†ç¦»VAEè¾“å‡º  
x=z.detach()                   # ðŸ”¥æ ¸å¿ƒï¼šåˆ†ç¦»VAEæ½œåœ¨è¡¨ç¤ºï¼Œé˜»æ­¢æ‰©æ•£æŸå¤±å›žä¼ åˆ°VAE

# 3. SiTè®­ç»ƒæ—¶é‡æ–°å¯ç”¨æ¢¯åº¦
requires_grad(model, True)     # é‡æ–°å¯ç”¨SiTå‚æ•°æ¢¯åº¦
model.train()                  # åˆ‡æ¢åˆ°è®­ç»ƒæ¨¡å¼

# è¿™æ ·ç¡®ä¿ï¼š
# - VAEè®­ç»ƒï¼šåªæ›´æ–°VAEå‚æ•°ï¼ŒSiTæä¾›ç‰¹å¾ä½†ä¸æ›´æ–°
# - SiTè®­ç»ƒï¼šåªæ›´æ–°SiTå‚æ•°ï¼ŒVAEæä¾›æ½œåœ¨è¡¨ç¤ºä½†ä¸æ›´æ–°
# - ä¸¤è€…é€šè¿‡REPAå¯¹é½æŸå¤±ååŒä¼˜åŒ–ï¼Œä½†é¿å…äº†æœ‰å®³çš„æ¢¯åº¦æµåŠ¨
```

### 5.6 è®­ç»ƒè¿‡ç¨‹ç›‘æŽ§

```python
# === è®­ç»ƒæ—¥å¿—è®°å½• ===
logs = {
    # SiTç›¸å…³æŸå¤±
    "sit_loss": sit_loss.item(),
    "denoising_loss": sit_outputs["denoising_loss"].mean().item(),
    "proj_loss": sit_outputs["proj_loss"].mean().item(),
    
    # VAEç›¸å…³æŸå¤±
    "vae_loss": vae_loss.item(),
    "reconstruction_loss": vae_loss_dict["reconstruction_loss"].mean().item(),
    "perceptual_loss": vae_loss_dict["perceptual_loss"].mean().item(),
    "kl_loss": vae_loss_dict["kl_loss"].mean().item(),
    "weighted_gan_loss": vae_loss_dict["weighted_gan_loss"].mean().item(),
    "vae_align_loss": vae_align_outputs["proj_loss"].mean().item(),
    
    # åˆ¤åˆ«å™¨æŸå¤±
    "d_loss": d_loss.item(),
    
    # æ¢¯åº¦èŒƒæ•°ç›‘æŽ§
    "grad_norm_sit": grad_norm_sit.item(),
    "grad_norm_vae": grad_norm_vae.item(), 
    "grad_norm_disc": grad_norm_disc.item(),
}
```

---

## 6. EMAæŒ‡æ•°ç§»åŠ¨å¹³å‡æ›´æ–°æœºåˆ¶

EMAæœºåˆ¶å·²ç»åœ¨ä¸Šé¢çš„ç«¯åˆ°ç«¯è®­ç»ƒå¾ªçŽ¯ä¸­è¯¦ç»†ä»‹ç»ï¼Œè¿™é‡Œè¡¥å……å…³é”®çš„åˆå§‹åŒ–å’Œä½¿ç”¨ç»†èŠ‚ï¼š

### 6.1 EMAåˆå§‹åŒ–

```python
# ===== æ–‡ä»¶: train_repae.py =====

# åˆ›å»ºSiTæ¨¡åž‹çš„EMAå‰¯æœ¬
model = model.to(device)
ema = copy.deepcopy(model).to(device)  # åˆ›å»ºæ¨¡åž‹çš„EMAå‰¯æœ¬

# å‡†å¤‡æ¨¡åž‹è¿›è¡Œè®­ç»ƒï¼šç¡®ä¿EMAä¸Žä¸»æ¨¡åž‹æƒé‡åŒæ­¥
update_ema(ema, model, decay=0)  # decay=0 æ„å‘³ç€å®Œå…¨å¤åˆ¶æƒé‡

# è®¾ç½®æ¨¡åž‹ä¸ºè¯„ä¼°æ¨¡å¼
model.eval()
ema.eval()
vae.eval()
```

### 6.2 è®­ç»ƒä¸­çš„EMAæ›´æ–°æ—¶æœº

```python
# EMAæ›´æ–°åªåœ¨æ¯ä¸ªgradient accumulationæ­¥éª¤åŽæ‰§è¡Œ
if accelerator.sync_gradients:
    unwrapped_model = accelerator.unwrap_model(model)
    # å¤„ç†ç¼–è¯‘æ¨¡åž‹çš„æƒ…å†µ
    original_model = unwrapped_model._orig_mod if args.compile else unwrapped_model
    update_ema(ema, original_model)
```

---

## 7. æ•°æ®é¢„å¤„ç†å’Œå½’ä¸€åŒ–å‡½æ•°

### 7.1 VAEå›¾åƒé¢„å¤„ç†

ä½äºŽ `utils.py` çš„å…³é”®é¢„å¤„ç†å‡½æ•°ï¼š

```python
# ===== æ–‡ä»¶: utils.py =====

def preprocess_imgs_vae(imgs):
    """VAEå›¾åƒé¢„å¤„ç†å‡½æ•° - å°†å›¾åƒä»Ž[0,255]è½¬æ¢ä¸º[-1,1]
    
    è¿™æ˜¯VAEè®­ç»ƒçš„æ ‡å‡†é¢„å¤„ç†ï¼Œç¡®ä¿è¾“å…¥èŒƒå›´ä¸ŽVAEè®­ç»ƒæ—¶ä¸€è‡´
    
    Args:
        imgs: è¾“å…¥å›¾åƒå¼ é‡ [B, C, H, W]ï¼Œå€¼åŸŸ[0, 255]ï¼Œæ•°æ®ç±»åž‹é€šå¸¸ä¸ºuint8
    
    Returns:
        å¤„ç†åŽçš„å›¾åƒå¼ é‡ [B, C, H, W]ï¼Œå€¼åŸŸ[-1, 1]ï¼Œæ•°æ®ç±»åž‹float32
    """
    return imgs.float() / 127.5 - 1.0
    # ç­‰ä»·äºŽï¼š(imgs.float() / 255.0) * 2.0 - 1.0
    # [0, 255] -> [0, 1] -> [0, 2] -> [-1, 1]
```

### 7.2 å›¾åƒè£å‰ªå‡½æ•°

```python
def center_crop_arr(pil_image, image_size):
    """ä¸­å¿ƒè£å‰ªå‡½æ•° - ä»ŽPILå›¾åƒä¸­å¿ƒè£å‰ªæŒ‡å®šå¤§å°
    
    Args:
        pil_image: PIL Imageå¯¹è±¡
        image_size: ç›®æ ‡å›¾åƒå°ºå¯¸ (æ­£æ–¹å½¢)
        
    Returns:
        è£å‰ªåŽçš„numpyæ•°ç»„
    """
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    arr = np.array(pil_image)
    
    # è®¡ç®—è£å‰ªä½ç½®ï¼ˆä¸­å¿ƒè£å‰ªï¼‰
    crop_y = (arr.shape[0] - image_size) // 2
    crop_x = (arr.shape[1] - image_size) // 2
    
    # æ‰§è¡Œè£å‰ª
    return arr[crop_y: crop_y + image_size, crop_x: crop_x + image_size]
```

### 7.3 æ•°æ®é›†åŠ è½½

```python
# ===== æ–‡ä»¶: train_repae.py =====

# æ•°æ®é›†è®¾ç½®
train_dataset = CustomINH5Dataset(args.data_dir)  # ImageNet H5æ•°æ®é›†
local_batch_size = int(args.batch_size // accelerator.num_processes)

train_dataloader = DataLoader(
    train_dataset,
    batch_size=local_batch_size,
    shuffle=True,
    num_workers=args.num_workers,
    pin_memory=True,        # å›ºå®šå†…å­˜ï¼ŒåŠ é€ŸGPUä¼ è¾“
    drop_last=True          # ä¸¢å¼ƒæœ€åŽä¸å®Œæ•´çš„æ‰¹æ¬¡
)
```

---

## 8. è§†è§‰ç¼–ç å™¨åŠ è½½å’Œç®¡ç†ç³»ç»Ÿ

### 8.1 æ”¯æŒçš„è§†è§‰ç¼–ç å™¨ç±»åž‹

```python
# ===== æ–‡ä»¶: utils.py =====

@torch.no_grad()
def load_encoders(enc_type, device, resolution=256):
    """åŠ è½½è§†è§‰ç¼–ç å™¨ - æ”¯æŒå¤šç§é¢„è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨
    
    Args:
        enc_type: ç¼–ç å™¨ç±»åž‹ï¼Œæ ¼å¼ä¸º "encoder_type-architecture-model_config"
                 ä¾‹å¦‚: "dinov2-vit-b", "clip-vit-L", "mocov3-vit-b"
        device: è®¡ç®—è®¾å¤‡
        resolution: è¾“å…¥å›¾åƒåˆ†è¾¨çŽ‡ (256 æˆ– 512)
        
    Returns:
        encoders: ç¼–ç å™¨æ¨¡åž‹åˆ—è¡¨
        architectures: æž¶æž„åˆ—è¡¨  
        encoder_types: ç¼–ç å™¨ç±»åž‹åˆ—è¡¨
        
    æ”¯æŒçš„ç¼–ç å™¨ï¼š
        - DINOv2: dinov2-vit-{b,l,g}
        - DINOv1: dinov1-vit-b  
        - CLIP: clip-vit-L
        - MoCov3: mocov3-vit-{s,b,l}
        - I-JEPA: jepa-vit-h
        - MAE: mae-vit-l
    """
    assert (resolution == 256) or (resolution == 512)
    
    enc_names = enc_type.split(',')  # æ”¯æŒå¤šä¸ªç¼–ç å™¨
    encoders, architectures, encoder_types = [], [], []
    
    for enc_name in enc_names:
        encoder_type, architecture, model_config = enc_name.split('-')
        
        # 512åˆ†è¾¨çŽ‡ç›®å‰åªæ”¯æŒDINOv2
        if resolution == 512:
            if encoder_type != 'dinov2':
                raise NotImplementedError(
                    "Currently, we only support 512x512 experiments with DINOv2 encoders."
                )

        architectures.append(architecture)
        encoder_types.append(encoder_type)
        
        # === MoCov3ç¼–ç å™¨ ===
        if encoder_type == 'mocov3':
            if architecture == 'vit':
                if model_config == 's':
                    encoder = mocov3_vit.vit_small()
                elif model_config == 'b': 
                    encoder = mocov3_vit.vit_base()
                elif model_config == 'l':
                    encoder = mocov3_vit.vit_large()
                else:
                    raise ValueError(f"Unsupported MoCov3 config: {model_config}")
                    
                # åŠ è½½é¢„è®­ç»ƒæƒé‡
                checkpoint_path = f"pretrained/mocov3_vit_{model_config}.pth"
                checkpoint = torch.load(checkpoint_path, map_location='cpu')
                state_dict = fix_mocov3_state_dict(checkpoint['state_dict'])
                encoder.load_state_dict(state_dict, strict=False)
                
        # === DINOv2ç¼–ç å™¨ ===
        elif encoder_type == 'dinov2':
            if architecture == 'vit':
                model_name = f'dinov2_vit{model_config}14'
                if resolution == 256:
                    model_name += '_reg'  # ä½¿ç”¨registerç‰ˆæœ¬ç”¨äºŽ256åˆ†è¾¨çŽ‡
                encoder = torch.hub.load('facebookresearch/dinov2', model_name)
                
        # === CLIPç¼–ç å™¨ ===
        elif encoder_type == 'clip':
            if architecture == 'vit' and model_config == 'L':
                import open_clip
                encoder, _, preprocess = open_clip.create_model_and_transforms(
                    'ViT-L-14', pretrained='laion2b_s32b_b82k'
                )
                encoder = encoder.visual  # åªä½¿ç”¨è§†è§‰ç¼–ç å™¨éƒ¨åˆ†
                
        # === I-JEPAç¼–ç å™¨ ===
        elif encoder_type == 'jepa':
            if architecture == 'vit' and model_config == 'h':
                from models.jepa import vit_huge
                encoder = vit_huge()
                checkpoint_path = "pretrained/jepa_vit_h.pth"
                checkpoint = torch.load(checkpoint_path, map_location='cpu')
                encoder.load_state_dict(checkpoint, strict=False)
                
        # === MAEç¼–ç å™¨ ===
        elif encoder_type == 'mae':
            if architecture == 'vit' and model_config == 'l':
                import timm
                encoder = timm.create_model('vit_large_patch16_224.mae', pretrained=True)
                
        else:
            raise ValueError(f"Unsupported encoder type: {encoder_type}")
        
        encoder = encoder.to(device).eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        encoders.append(encoder)
    
    return encoders, architectures, encoder_types
```

### 8.2 MoCov3çŠ¶æ€å­—å…¸ä¿®å¤

```python
def fix_mocov3_state_dict(state_dict):
    """ä¿®å¤MoCov3æ£€æŸ¥ç‚¹çš„çŠ¶æ€å­—å…¸
    
    MoCov3æ£€æŸ¥ç‚¹åŒ…å«ä¸€äº›å‘½åé”™è¯¯ï¼Œéœ€è¦ä¿®å¤æ‰èƒ½æ­£ç¡®åŠ è½½
    
    Args:
        state_dict: åŽŸå§‹çŠ¶æ€å­—å…¸
        
    Returns:
        ä¿®å¤åŽçš„çŠ¶æ€å­—å…¸
    """
    for k in list(state_dict.keys()):
        # åªä¿ç•™base_encoderçš„å‚æ•°
        if k.startswith('module.base_encoder'):
            # ä¿®å¤å‘½åé”™è¯¯
            new_k = k[len("module.base_encoder."):]
            
            # ä¿®å¤ç‰¹å®šçš„å‘½åé”™è¯¯
            if "blocks.13.norm13" in new_k:
                new_k = new_k.replace("norm13", "norm1")
            if "blocks.13.mlp.fc13" in k:
                new_k = new_k.replace("fc13", "fc1")
            if "blocks.14.norm14" in k:
                new_k = new_k.replace("norm14", "norm2")  
            if "blocks.14.mlp.fc14" in k:
                new_k = new_k.replace("fc14", "fc2")
            
            # ç§»é™¤å‰ç¼€ï¼Œä¿ç•™æœ‰æ•ˆå‚æ•°
            if 'head' not in new_k and new_k.split('.')[0] != 'fc':
                state_dict[new_k] = state_dict[k]
        
        # åˆ é™¤åŽŸå§‹é”®
        del state_dict[k]
    
    # è°ƒæ•´ä½ç½®ç¼–ç å°ºå¯¸
    if 'pos_embed' in state_dict.keys():
        state_dict['pos_embed'] = timm.layers.pos_embed.resample_abs_pos_embed(
            state_dict['pos_embed'], [16, 16],
        )
    
    return state_dict
```

### 8.3 ç‰¹å¾æå–å’Œå½’ä¸€åŒ–

```python
def extract_visual_features(encoders, images, encoder_types):
    """ä»Žè§†è§‰ç¼–ç å™¨æå–ç‰¹å¾
    
    Args:
        encoders: ç¼–ç å™¨æ¨¡åž‹åˆ—è¡¨
        images: è¾“å…¥å›¾åƒ [B, C, H, W]
        encoder_types: ç¼–ç å™¨ç±»åž‹åˆ—è¡¨
        
    Returns:
        zs: ç‰¹å¾åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º [B, N, D]
    """
    zs = []
    
    for encoder, encoder_type in zip(encoders, encoder_types):
        with torch.no_grad():
            if encoder_type in ['dinov2', 'dinov1', 'mae']:
                # Vision Transformerç¼–ç å™¨
                features = encoder.forward_features(images)
                if hasattr(encoder, 'norm'):
                    features = encoder.norm(features)
                    
            elif encoder_type == 'clip':
                # CLIPè§†è§‰ç¼–ç å™¨
                features = encoder(images)
                
            elif encoder_type == 'mocov3':
                # MoCov3ç¼–ç å™¨
                features = encoder(images)
                
            elif encoder_type == 'jepa':
                # I-JEPAç¼–ç å™¨
                features = encoder(images)
            
            # ç¡®ä¿ç‰¹å¾ä¸º[B, N, D]æ ¼å¼
            if len(features.shape) == 3:  # [B, N, D]
                zs.append(features)
            else:  # [B, D] -> [B, 1, D]  
                zs.append(features.unsqueeze(1))
    
    return zs

def count_trainable_params(model):
    """è®¡ç®—æ¨¡åž‹çš„å¯è®­ç»ƒå‚æ•°æ•°é‡"""
    return sum(p.numel() for p in model.parameters() if p.requires_grad)
```

---

## 9. é‡‡æ ·å™¨å’Œç”Ÿæˆç­–ç•¥

### 9.1 æ ¸å¿ƒé‡‡æ ·å™¨å®žçŽ°

REPA-Eä½¿ç”¨å¤šç§é‡‡æ ·ç­–ç•¥è¿›è¡Œå›¾åƒç”Ÿæˆï¼Œä½äºŽ `samplers.py`ï¼š

```python
# ===== æ–‡ä»¶: samplers.py =====

import torch
import numpy as np

def expand_t_like_x(t, x_cur):
    """å°†æ—¶é—´té‡å¡‘ä¸ºå¯å¹¿æ’­åˆ°xç»´åº¦çš„å½¢çŠ¶
    
    Args:
        t: æ—¶é—´å‘é‡ [batch_dim,]
        x_cur: æ•°æ®ç‚¹ [batch_dim, ...]
        
    Returns:
        é‡å¡‘åŽçš„æ—¶é—´å¼ é‡ï¼Œå¯ä¸Žx_curå¹¿æ’­
    """
    dims = [1] * (len(x_cur.size()) - 1)
    t = t.view(t.size(0), *dims)
    return t

def get_score_from_velocity(vt, xt, t, path_type="linear"):
    """ä»Žé€Ÿåº¦é¢„æµ‹è½¬æ¢ä¸ºåˆ†æ•°å‡½æ•° (å…³é”®è½¬æ¢å‡½æ•°)
    
    å°†SiTçš„velocity predictionè½¬æ¢ä¸ºscore-based modelçš„åˆ†æ•°å‡½æ•°
    
    Args:
        vt: é€Ÿåº¦æ¨¡åž‹è¾“å‡º [batch_dim, ...]
        xt: å½“å‰æ•°æ®ç‚¹ [batch_dim, ...]
        t: æ—¶é—´æ­¥ [batch_dim,]
        path_type: è·¯å¾„ç±»åž‹ ("linear" or "cosine")
        
    Returns:
        score: åˆ†æ•°å‡½æ•°å€¼ [batch_dim, ...]
    """
    t = expand_t_like_x(t, xt)
    
    # æ ¹æ®è·¯å¾„ç±»åž‹è®¡ç®—æ’å€¼ç³»æ•°
    if path_type == "linear":
        alpha_t = 1 - t                                    # æ•°æ®ç³»æ•°
        sigma_t = t                                        # å™ªå£°ç³»æ•° 
        d_alpha_t = torch.ones_like(xt, device=xt.device) * -1  # alphaå¯¼æ•°
        d_sigma_t = torch.ones_like(xt, device=xt.device)       # sigmaå¯¼æ•°
    elif path_type == "cosine":
        alpha_t = torch.cos(t * np.pi / 2)
        sigma_t = torch.sin(t * np.pi / 2)
        d_alpha_t = -np.pi / 2 * torch.sin(t * np.pi / 2)
        d_sigma_t = np.pi / 2 * torch.cos(t * np.pi / 2)
    else:
        raise NotImplementedError(f"Path type {path_type} not implemented")

    # è®¡ç®—åˆ†æ•°å‡½æ•°
    mean = xt
    reverse_alpha_ratio = alpha_t / d_alpha_t
    var = sigma_t**2 - reverse_alpha_ratio * d_sigma_t * sigma_t
    score = (reverse_alpha_ratio * vt - mean) / var

    return score

def compute_diffusion(t_cur):
    """è®¡ç®—æ‰©æ•£ç³»æ•°
    
    Args:
        t_cur: å½“å‰æ—¶é—´æ­¥
        
    Returns:
        æ‰©æ•£ç³»æ•°
    """
    return 2 * t_cur
```

### 9.2 Euleré‡‡æ ·å™¨

```python
def euler_sampler(
    model,
    latents,
    y,
    num_steps=20,
    heun=False,
    cfg_scale=1.0,
    guidance_low=0.0,
    guidance_high=1.0,
    path_type="linear",  # å…¼å®¹æ€§å‚æ•°
):
    """Euleré‡‡æ ·å™¨ - REPA-Eçš„ä¸»è¦é‡‡æ ·æ–¹æ³•
    
    ä½¿ç”¨Euleræ–¹æ³•æ±‚è§£åå‘SDEï¼Œä»Žå™ªå£°ç”Ÿæˆå›¾åƒ
    
    Args:
        model: è®­ç»ƒå¥½çš„SiTæ¨¡åž‹
        latents: åˆå§‹å™ªå£° [B, C, H, W]
        y: ç±»åˆ«æ ‡ç­¾ [B,]
        num_steps: é‡‡æ ·æ­¥æ•° (é»˜è®¤20)
        heun: æ˜¯å¦ä½¿ç”¨Heunæ–¹æ³•æé«˜ç²¾åº¦
        cfg_scale: Classifier-free guidanceå°ºåº¦
        guidance_low/high: å¼•å¯¼åº”ç”¨çš„æ—¶é—´èŒƒå›´
        path_type: è·¯å¾„ç±»åž‹ (å…¼å®¹æ€§ï¼Œå®žé™…æœªä½¿ç”¨)
        
    Returns:
        ç”Ÿæˆçš„æ½œåœ¨è¡¨ç¤º [B, C, H, W]
    """
    # === è®¾ç½®Classifier-free guidance ===
    if cfg_scale > 1.0:
        y_null = torch.tensor([1000] * y.size(0), device=y.device)  # æ— æ¡ä»¶ç±»åˆ«(ImageNetæœ‰1000ç±»)
        
    _dtype = latents.dtype
    device = latents.device
    
    # === æ—¶é—´æ­¥è®¾ç½® ===
    t_steps = torch.linspace(1, 0, num_steps + 1, dtype=torch.float64)  # [1.0, ..., 0.0]
    x_next = latents.to(torch.float64)

    with torch.no_grad():
        for i, (t_cur, t_next) in enumerate(zip(t_steps[:-1], t_steps[1:])):
            x_cur = x_next
            
            # === Classifier-free guidanceè®¾ç½® ===
            if cfg_scale > 1.0 and guidance_low <= t_cur <= guidance_high:
                # åŒæ—¶è®¡ç®—æœ‰æ¡ä»¶å’Œæ— æ¡ä»¶é¢„æµ‹
                model_input = torch.cat([x_cur] * 2, dim=0)
                y_cur = torch.cat([y, y_null], dim=0)
            else:
                model_input = x_cur
                y_cur = y
                
            # === æ¨¡åž‹æŽ¨ç† ===
            kwargs = dict(y=y_cur)
            time_input = torch.ones(model_input.size(0), device=device, dtype=torch.float64) * t_cur
            
            # èŽ·å–velocity prediction
            d_cur = model.inference(
                model_input.to(dtype=_dtype), 
                time_input.to(dtype=_dtype), 
                **kwargs
            ).to(torch.float64)
            
            # === Classifier-free guidanceåº”ç”¨ ===
            if cfg_scale > 1.0 and guidance_low <= t_cur <= guidance_high:
                d_cur_cond, d_cur_uncond = d_cur.chunk(2)
                # CFGå…¬å¼: uncond + scale * (cond - uncond)
                d_cur = d_cur_uncond + cfg_scale * (d_cur_cond - d_cur_uncond)
            
            # === Euleræ­¥éª¤æ›´æ–° ===
            x_next = x_cur + (t_next - t_cur) * d_cur
            
            # === å¯é€‰çš„Heunæ ¡æ­£ (æé«˜é‡‡æ ·ç²¾åº¦) ===
            if heun and (i < num_steps - 1):
                # ä½¿ç”¨æ›´æ–°åŽçš„x_nextå†æ¬¡é¢„æµ‹
                if cfg_scale > 1.0 and guidance_low <= t_cur <= guidance_high:
                    model_input = torch.cat([x_next] * 2)
                    y_cur = torch.cat([y, y_null], dim=0)
                else:
                    model_input = x_next
                    y_cur = y
                    
                kwargs = dict(y=y_cur)
                time_input = torch.ones(model_input.size(0), device=device, dtype=torch.float64) * t_next
                
                d_prime = model.inference(
                    model_input.to(dtype=_dtype), 
                    time_input.to(dtype=_dtype), 
                    **kwargs
                ).to(torch.float64)
                
                if cfg_scale > 1.0 and guidance_low <= t_cur <= guidance_high:
                    d_prime_cond, d_prime_uncond = d_prime.chunk(2)
                    d_prime = d_prime_uncond + cfg_scale * (d_prime_cond - d_prime_uncond)
                
                # Heunæ ¡æ­£: ä½¿ç”¨ä¸¤æ¬¡é¢„æµ‹çš„å¹³å‡å€¼
                x_next = x_cur + (t_next - t_cur) * (0.5 * d_cur + 0.5 * d_prime)

    return x_next
```

### 9.3 Euler-Maruyamaé‡‡æ ·å™¨ (SDEé‡‡æ ·)

```python
def euler_maruyama_sampler(
    model,
    latents,
    y,
    num_steps=20,
    heun=False,      # å…¼å®¹æ€§å‚æ•°ï¼Œæœªä½¿ç”¨
    cfg_scale=1.0,
    guidance_low=0.0,
    guidance_high=1.0,
    path_type="linear",
):
    """Euler-Maruyamaé‡‡æ ·å™¨ - éšæœºå¾®åˆ†æ–¹ç¨‹é‡‡æ ·
    
    ä½¿ç”¨Euler-Maruyamaæ–¹æ³•æ±‚è§£SDEï¼ŒåŒ…å«éšæœºæ€§
    
    Args:
        model: è®­ç»ƒå¥½çš„SiTæ¨¡åž‹
        latents: åˆå§‹å™ªå£° [B, C, H, W]
        y: ç±»åˆ«æ ‡ç­¾ [B,]
        num_steps: é‡‡æ ·æ­¥æ•°
        cfg_scale: Classifier-free guidanceå°ºåº¦
        guidance_low/high: å¼•å¯¼åº”ç”¨çš„æ—¶é—´èŒƒå›´
        path_type: è·¯å¾„ç±»åž‹ ("linear" or "cosine")
        
    Returns:
        ç”Ÿæˆçš„æ½œåœ¨è¡¨ç¤º [B, C, H, W]
    """
    # === è®¾ç½®Classifier-free guidance ===
    if cfg_scale > 1.0:
        y_null = torch.tensor([1000] * y.size(0), device=y.device)
        
    _dtype = latents.dtype
    device = latents.device
    
    # === æ—¶é—´æ­¥è®¾ç½® (ä¸åŒ…å«0ï¼Œé¿å…æ•°å€¼é—®é¢˜) ===
    t_steps = torch.linspace(1.0, 0.04, num_steps, dtype=torch.float64)
    t_steps = torch.cat([t_steps, torch.tensor([0.0], dtype=torch.float64)])
    x_next = latents.to(torch.float64)

    with torch.no_grad():
        for i, (t_cur, t_next) in enumerate(zip(t_steps[:-2], t_steps[1:-1])):
            dt = t_next - t_cur  # æ—¶é—´æ­¥é•¿
            x_cur = x_next
            
            # === Classifier-free guidanceè®¾ç½® ===
            if cfg_scale > 1.0 and guidance_low <= t_cur <= guidance_high:
                model_input = torch.cat([x_cur] * 2, dim=0)
                y_cur = torch.cat([y, y_null], dim=0)
            else:
                model_input = x_cur
                y_cur = y
                
            # === éšæœºé¡¹ ===
            kwargs = dict(y=y_cur)
            time_input = torch.ones(model_input.size(0), device=device, dtype=torch.float64) * t_cur
            diffusion = compute_diffusion(t_cur)  # æ‰©æ•£ç³»æ•°
            eps_i = torch.randn_like(x_cur, device=device)
            deps = eps_i * torch.sqrt(torch.abs(dt))  # éšæœºæ‰°åŠ¨
            
            # === è®¡ç®—drifté¡¹ ===
            # èŽ·å–velocity prediction
            v_cur = model.inference(
                model_input.to(dtype=_dtype), 
                time_input.to(dtype=_dtype), 
                **kwargs
            ).to(torch.float64)
            
            # è½¬æ¢ä¸ºscoreå‡½æ•°
            s_cur = get_score_from_velocity(v_cur, model_input, time_input, path_type=path_type)
            
            # è®¡ç®—drift: v - 0.5 * g^2 * score  
            d_cur = v_cur - 0.5 * diffusion * s_cur
            
            # === Classifier-free guidance ===
            if cfg_scale > 1.0 and guidance_low <= t_cur <= guidance_high:
                d_cur_cond, d_cur_uncond = d_cur.chunk(2)
                s_cur_cond, s_cur_uncond = s_cur.chunk(2)
                
                # å¯¹driftå’Œscoreéƒ½åº”ç”¨CFG
                d_cur = d_cur_uncond + cfg_scale * (d_cur_cond - d_cur_uncond)
                s_cur = s_cur_uncond + cfg_scale * (s_cur_cond - s_cur_uncond)
            
            # === Euler-Maruyamaæ›´æ–° ===
            # dx = drift * dt + diffusion * score * dt + diffusion * dW
            x_next = (x_cur + 
                     d_cur * dt +                    # drifté¡¹
                     diffusion * s_cur * dt +        # scoreé¡¹  
                     torch.sqrt(diffusion) * deps)   # éšæœºé¡¹

    return x_next
```

### 9.4 é‡‡æ ·å™¨ä½¿ç”¨ç¤ºä¾‹

```python
# === åœ¨generate.pyä¸­çš„ä½¿ç”¨ç¤ºä¾‹ ===

def generate_images(model, vae, num_samples=50000, cfg_scale=4.0):
    """ä½¿ç”¨è®­ç»ƒå¥½çš„REPA-Eæ¨¡åž‹ç”Ÿæˆå›¾åƒ"""
    
    # è®¾ç½®é‡‡æ ·å‚æ•°
    num_steps = 250          # é‡‡æ ·æ­¥æ•°
    guidance_high = 1.0      # é«˜æ—¶é—´æ­¥å¼•å¯¼
    guidance_low = 0.0       # ä½Žæ—¶é—´æ­¥å¼•å¯¼
    
    # ç”Ÿæˆéšæœºå™ªå£°å’Œç±»åˆ«
    device = next(model.parameters()).device
    latents = torch.randn(batch_size, in_channels, latent_size, latent_size, device=device)
    y = torch.randint(0, num_classes, (batch_size,), device=device)
    
    # === ä½¿ç”¨Euleré‡‡æ ·å™¨ç”Ÿæˆæ½œåœ¨è¡¨ç¤º ===
    with torch.no_grad():
        generated_latents = euler_sampler(
            model=model,
            latents=latents,
            y=y,
            num_steps=num_steps,
            heun=False,              # ä¸ä½¿ç”¨Heunæ ¡æ­£
            cfg_scale=cfg_scale,     # Classifier-free guidance
            guidance_low=guidance_low,
            guidance_high=guidance_high,
            path_type="linear"
        )
        
        # VAEè§£ç ä¸ºå›¾åƒ
        generated_images = vae.decode(generated_latents)
        
        # è½¬æ¢åˆ°[0,1]èŒƒå›´
        generated_images = (generated_images + 1) / 2
        generated_images = torch.clamp(generated_images, 0, 1)
    
    return generated_images

# === é‡‡æ ·å™¨å‚æ•°è¯´æ˜Ž ===
sampling_configs = {
    # åŸºç¡€å‚æ•°
    "num_steps": 250,           # é‡‡æ ·æ­¥æ•°ï¼Œæ›´å¤šæ­¥æ•°=æ›´é«˜è´¨é‡ä½†æ›´æ…¢
    "cfg_scale": 4.0,          # CFGå°ºåº¦ï¼Œ1.0=æ— å¼•å¯¼ï¼Œ>1.0=æœ‰æ¡ä»¶å¼•å¯¼
    "heun": False,             # Heunæ ¡æ­£ï¼Œæé«˜ç²¾åº¦ä½†å¢žåŠ è®¡ç®—
    
    # å¼•å¯¼æŽ§åˆ¶
    "guidance_low": 0.0,       # å¼•å¯¼åº”ç”¨çš„æœ€ä½Žæ—¶é—´æ­¥
    "guidance_high": 1.0,      # å¼•å¯¼åº”ç”¨çš„æœ€é«˜æ—¶é—´æ­¥
    
    # è·¯å¾„å‚æ•°  
    "path_type": "linear",     # æ’å€¼è·¯å¾„ç±»åž‹ (linear/cosine)
}
```

### 9.5 ä¸åŒé‡‡æ ·å™¨çš„ç‰¹ç‚¹å¯¹æ¯”

```python
# === é‡‡æ ·å™¨é€‰æ‹©æŒ‡å— ===

sampling_methods = {
    "euler_sampler": {
        "ç±»åž‹": "ODEæ±‚è§£å™¨",
        "ç‰¹ç‚¹": "ç¡®å®šæ€§é‡‡æ ·ï¼Œç»“æžœå¯å¤çŽ°", 
        "é€‚ç”¨": "é«˜è´¨é‡å›¾åƒç”Ÿæˆï¼Œè¯„ä¼°æŒ‡æ ‡è®¡ç®—",
        "é€Ÿåº¦": "å¿«",
        "è´¨é‡": "é«˜"
    },
    
    "euler_maruyama_sampler": {
        "ç±»åž‹": "SDEæ±‚è§£å™¨", 
        "ç‰¹ç‚¹": "éšæœºæ€§é‡‡æ ·ï¼Œæ¯æ¬¡ç»“æžœä¸åŒ",
        "é€‚ç”¨": "å¤šæ ·æ€§ç”Ÿæˆï¼ŒæŽ¢ç´¢æ¨¡å¼ç©ºé—´",
        "é€Ÿåº¦": "ä¸­ç­‰",
        "è´¨é‡": "ä¸­ç­‰åˆ°é«˜"
    }
}

# æŽ¨èä½¿ç”¨åœºæ™¯ï¼š
# - è®ºæ–‡è¯„ä¼°ã€FIDè®¡ç®—: euler_sampler (ç¡®å®šæ€§ï¼Œå¯å¤çŽ°)
# - åˆ›æ„ç”Ÿæˆã€å¤šæ ·æ€§: euler_maruyama_sampler (éšæœºæ€§)
# - å¿«é€Ÿé¢„è§ˆ: å‡å°‘num_steps (50-100æ­¥)
# - é«˜è´¨é‡ç”Ÿæˆ: å¢žåŠ num_steps (250-500æ­¥)
```

---

## 10. å‚æ•°é…ç½®å’Œå…³é”®è¶…å‚æ•°è®¾ç½®

### 10.1 æŸå¤±å‡½æ•°é…ç½®æ–‡ä»¶

ä½äºŽ `configs/l1_lpips_kl_gan.yaml`ï¼š

```yaml
# ===== æ–‡ä»¶: configs/l1_lpips_kl_gan.yaml =====

model:
  vq_model:
    quantize_mode: vae  # ä½¿ç”¨VAEæ¨¡å¼è€Œä¸æ˜¯VQ

losses:
  # === åˆ¤åˆ«å™¨è®­ç»ƒå‚æ•° ===
  discriminator_start: 0        # ä»Žç¬¬0æ­¥å¼€å§‹è®­ç»ƒåˆ¤åˆ«å™¨
  discriminator_factor: 1.0     # åˆ¤åˆ«å™¨æŸå¤±å› å­
  discriminator_weight: 0.1     # åˆ¤åˆ«å™¨æƒé‡ (ç›¸å¯¹äºŽé‡å»ºæŸå¤±)
  
  # === é‡åŒ–å™¨è®¾ç½® ===  
  quantizer_weight: 1.0         # é‡åŒ–å™¨æƒé‡ (VQæ¨¡å¼æ—¶ä½¿ç”¨)
  
  # === æ„ŸçŸ¥æŸå¤±è®¾ç½® ===
  perceptual_loss: "lpips"      # ä½¿ç”¨LPIPSæ„ŸçŸ¥æŸå¤±
  perceptual_weight: 1.0        # LPIPSæƒé‡
  
  # === é‡å»ºæŸå¤±è®¾ç½® ===
  reconstruction_loss: "l1"     # L1é‡å»ºæŸå¤±
  reconstruction_weight: 1.0    # é‡å»ºæŸå¤±æƒé‡
  
  # === LeCAMæ­£åˆ™åŒ– ===
  lecam_regularization_weight: 0.0  # LeCAMæ­£åˆ™åŒ–æƒé‡ (å…³é—­)
  
  # === KLæ•£åº¦è®¾ç½® ===
  kl_weight: 1e-6              # ðŸ”¥å…³é”®ï¼šKLæ•£åº¦æƒé‡
  logvar_init: 0.0             # log varianceåˆå§‹åŒ–å€¼
```

### 10.2 REPA-Eè®­ç»ƒå…³é”®è¶…å‚æ•°

```python
# ===== æ ¸å¿ƒè®­ç»ƒè¶…å‚æ•°æ±‡æ€» =====

# === æŠ•å½±å¯¹é½æŸå¤±ç³»æ•° ===
PROJ_COEFF = 0.5                    # SiTæŠ•å½±å¯¹é½æŸå¤±ç³»æ•°
VAE_ALIGN_PROJ_COEFF = 1.5          # VAEæŠ•å½±å¯¹é½æŸå¤±ç³»æ•°

# === å­¦ä¹ çŽ‡è®¾ç½® ===
LEARNING_RATE = 1e-4                # SiTå­¦ä¹ çŽ‡
VAE_LEARNING_RATE = 1e-4            # VAEå­¦ä¹ çŽ‡
DISC_LEARNING_RATE = 1e-4           # åˆ¤åˆ«å™¨å­¦ä¹ çŽ‡

# === ä¼˜åŒ–å™¨å‚æ•° ===
ADAM_BETA1 = 0.9                    # Adam beta1
ADAM_BETA2 = 0.999                  # Adam beta2  
ADAM_WEIGHT_DECAY = 0.0             # æƒé‡è¡°å‡
ADAM_EPSILON = 1e-8                 # æ•°å€¼ç¨³å®šæ€§

# === BatchNormå‚æ•° ===
BN_MOMENTUM = 0.1                   # ðŸ”¥å…³é”®ï¼šBatchNormåŠ¨é‡

# === EMAå‚æ•° ===
EMA_DECAY = 0.9999                  # EMAè¡°å‡ç³»æ•°

# === è®­ç»ƒå‚æ•° ===
BATCH_SIZE = 256                    # æ‰¹æ¬¡å¤§å°
MAX_TRAIN_STEPS = 400000            # æœ€å¤§è®­ç»ƒæ­¥æ•° (400K)
CHECKPOINTING_STEPS = 50000         # æ£€æŸ¥ç‚¹ä¿å­˜é—´éš”
MAX_GRAD_NORM = 1.0                 # æ¢¯åº¦è£å‰ªèŒƒæ•°

# === æ¨¡åž‹æž¶æž„å‚æ•° ===
MODEL_TYPE = "SiT-XL/2"            # SiTæ¨¡åž‹ç±»åž‹
VAE_TYPE = "f8d4"                   # VAEæž¶æž„ (8å€ä¸‹é‡‡æ ·ï¼Œ4é€šé“)
ENCODER_TYPE = "dinov2-vit-b"       # è§†è§‰ç¼–ç å™¨ç±»åž‹
ENCODER_DEPTH = 8                   # ç¼–ç å™¨ç‰¹å¾æå–æ·±åº¦

# === MLPæŠ•å½±å™¨å‚æ•° ===
PROJECTOR_DIM = 2048               # æŠ•å½±å™¨ä¸­é—´å±‚ç»´åº¦
Z_DIMS = [768]                     # ç›®æ ‡ç‰¹å¾ç»´åº¦åˆ—è¡¨ (DINOv2-B: 768)

# === æ‰©æ•£æ¨¡åž‹å‚æ•° ===
PATH_TYPE = "linear"               # æ’å€¼è·¯å¾„ç±»åž‹
PREDICTION = "v"                   # é¢„æµ‹ç±»åž‹ (velocity prediction)
WEIGHTING = "uniform"              # æŸå¤±æƒé‡æ–¹æ¡ˆ
CFG_PROB = 0.1                    # Classifier-free guidanceæ¦‚çŽ‡
```

### 10.3 å‘½ä»¤è¡Œå‚æ•°å®Œæ•´é…ç½®

```bash
# ===== REPA-Eè®­ç»ƒå®Œæ•´å‘½ä»¤ =====

accelerate launch train_repae.py \
    # === åŸºç¡€è®­ç»ƒå‚æ•° ===
    --max-train-steps=400000 \
    --report-to="wandb" \
    --allow-tf32 \
    --mixed-precision="fp16" \
    --seed=0 \
    --batch-size=256 \
    --num-workers=4 \
    
    # === æ•°æ®å’Œè¾“å‡º ===
    --data-dir="data" \
    --output-dir="exps" \
    --exp-name="sit-xl-dinov2-b-enc8-repae-sdvae-0.5-1.5-400k" \
    
    # === æ‰©æ•£æ¨¡åž‹å‚æ•° ===
    --path-type="linear" \
    --prediction="v" \
    --weighting="uniform" \
    --model="SiT-XL/2" \
    --checkpointing-steps=50000 \
    
    # === æŸå¤±é…ç½® ===
    --loss-cfg-path="configs/l1_lpips_kl_gan.yaml" \
    
    # === VAEè®¾ç½® ===
    --vae="f8d4" \
    --vae-ckpt="pretrained/sdvae/sdvae-f8d4.pt" \
    --disc-pretrained-ckpt="pretrained/sdvae/sdvae-f8d4-discriminator-ckpt.pt" \
    
    # === ðŸ”¥ REPA-Eæ ¸å¿ƒå‚æ•° ===
    --enc-type="dinov2-vit-b" \          # è§†è§‰ç¼–ç å™¨ç±»åž‹
    --proj-coeff=0.5 \                   # SiTæŠ•å½±å¯¹é½ç³»æ•°  
    --encoder-depth=8 \                  # ç‰¹å¾æå–æ·±åº¦
    --vae-align-proj-coeff=1.5 \         # VAEæŠ•å½±å¯¹é½ç³»æ•°
    --bn-momentum=0.1 \                  # BatchNormåŠ¨é‡
    
    # === å­¦ä¹ çŽ‡è®¾ç½® ===
    --learning-rate=1e-4 \               # SiTå­¦ä¹ çŽ‡
    --vae-learning-rate=1e-4 \           # VAEå­¦ä¹ çŽ‡
    --disc-learning-rate=1e-4 \          # åˆ¤åˆ«å™¨å­¦ä¹ çŽ‡
    
    # === ä¼˜åŒ–å™¨å‚æ•° ===
    --adam-beta1=0.9 \
    --adam-beta2=0.999 \
    --adam-weight-decay=0.0 \
    --adam-epsilon=1e-8 \
    --max-grad-norm=1.0
```

### 10.4 ä¸åŒVAEæž¶æž„çš„å‚æ•°é…ç½®

```python
# ===== VAEæž¶æž„é…ç½®å¯¹æ¯” =====

vae_configs = {
    "f8d4": {
        "description": "SD-VAEï¼Œ8å€ä¸‹é‡‡æ ·ï¼Œ4é€šé“",
        "patch_size": 2,           # SiT patch size
        "model_type": "SiT-XL/2",  # å¯¹åº”çš„SiTæ¨¡åž‹
        "latent_size": 32,         # 256/8 = 32
        "channels": 4,             # æ½œåœ¨ç©ºé—´é€šé“æ•°
        "proj_coeff": 0.5,         # æŽ¨èæŠ•å½±ç³»æ•°
        "vae_align_coeff": 1.5,    # æŽ¨èVAEå¯¹é½ç³»æ•°
    },
    
    "f16d32": {
        "description": "E2E-VAEï¼Œ16å€ä¸‹é‡‡æ ·ï¼Œ32é€šé“", 
        "patch_size": 1,           # SiT patch size
        "model_type": "SiT-XL/1",  # å¯¹åº”çš„SiTæ¨¡åž‹
        "latent_size": 16,         # 256/16 = 16
        "channels": 32,            # æ½œåœ¨ç©ºé—´é€šé“æ•°
        "proj_coeff": 0.5,         # æŽ¨èæŠ•å½±ç³»æ•°
        "vae_align_coeff": 1.0,    # æŽ¨èVAEå¯¹é½ç³»æ•°
    }
}

# === ç¼–ç å™¨ç‰¹å¾ç»´åº¦é…ç½® ===
encoder_dims = {
    "dinov2-vit-b": 768,      # DINOv2 Base
    "dinov2-vit-l": 1024,     # DINOv2 Large  
    "dinov2-vit-g": 1536,     # DINOv2 Giant
    "clip-vit-L": 768,        # CLIP Large
    "mocov3-vit-b": 768,      # MoCov3 Base
    "mocov3-vit-l": 1024,     # MoCov3 Large
    "jepa-vit-h": 1280,       # I-JEPA Huge
    "mae-vit-l": 1024,        # MAE Large
}
```

### 10.5 é‡‡æ ·å’Œç”Ÿæˆå‚æ•°é…ç½®

```python
# ===== ç”Ÿæˆé˜¶æ®µå‚æ•°é…ç½® =====

generation_configs = {
    # === é‡‡æ ·å™¨å‚æ•° ===
    "num_steps": 250,              # é‡‡æ ·æ­¥æ•°
    "cfg_scale": 4.0,              # Classifier-free guidanceå°ºåº¦
    "guidance_low": 0.0,           # å¼•å¯¼æœ€ä½Žæ—¶é—´æ­¥
    "guidance_high": 1.0,          # å¼•å¯¼æœ€é«˜æ—¶é—´æ­¥
    "heun": False,                 # æ˜¯å¦ä½¿ç”¨Heunæ ¡æ­£
    
    # === è¯„ä¼°å‚æ•° ===
    "num_fid_samples": 50000,      # FIDè®¡ç®—æ ·æœ¬æ•°
    "batch_size_eval": 64,         # è¯„ä¼°æ‰¹æ¬¡å¤§å°
    "resolution": 256,             # å›¾åƒåˆ†è¾¨çŽ‡
    
    # === ç”Ÿæˆè®¾ç½® ===
    "mode": "sde",                 # é‡‡æ ·æ¨¡å¼ ("ode" or "sde")
    "sampler": "euler",            # é‡‡æ ·å™¨ç±»åž‹
    "path_type": "linear",         # è·¯å¾„ç±»åž‹
}

# === å¿«é€Ÿé¢„è§ˆé…ç½® ===
preview_configs = {
    "num_steps": 50,               # å‡å°‘æ­¥æ•°åŠ é€Ÿ
    "cfg_scale": 2.0,              # é™ä½ŽCFGå°ºåº¦  
    "batch_size": 16,              # å°æ‰¹æ¬¡
    "num_samples": 64,             # å°‘é‡æ ·æœ¬
}

# === é«˜è´¨é‡ç”Ÿæˆé…ç½® ===
hq_configs = {
    "num_steps": 500,              # å¢žåŠ æ­¥æ•°æå‡è´¨é‡
    "cfg_scale": 6.0,              # æ›´å¼ºçš„æ¡ä»¶å¼•å¯¼
    "heun": True,                  # å¯ç”¨Heunæ ¡æ­£
    "batch_size": 8,               # å‡å°‘æ‰¹æ¬¡é¿å…OOM
}
```

### 10.6 è¶…å‚æ•°è°ƒä¼˜æŒ‡å—

```python
# ===== è¶…å‚æ•°è°ƒä¼˜å»ºè®® =====

tuning_guide = {
    # === æ ¸å¿ƒREPA-Eå‚æ•° ===
    "proj_coeff": {
        "range": [0.1, 1.0],
        "default": 0.5,
        "impact": "æŽ§åˆ¶SiTæŠ•å½±å¯¹é½å¼ºåº¦",
        "è°ƒä¼˜": "è¿‡å¤§å¯èƒ½å½±å“åŽ»å™ªæ€§èƒ½ï¼Œè¿‡å°å¯¹é½æ•ˆæžœå·®"
    },
    
    "vae_align_proj_coeff": {
        "range": [0.5, 3.0], 
        "default": 1.5,
        "impact": "æŽ§åˆ¶VAEæŠ•å½±å¯¹é½å¼ºåº¦",
        "è°ƒä¼˜": "é€šå¸¸è®¾ç½®ä¸ºproj_coeffçš„2-3å€"
    },
    
    "bn_momentum": {
        "range": [0.01, 0.3],
        "default": 0.1, 
        "impact": "BatchNormç»Ÿè®¡é‡æ›´æ–°é€Ÿåº¦",
        "è°ƒä¼˜": "è¾ƒå°å€¼æ›´ç¨³å®šï¼Œè¾ƒå¤§å€¼é€‚åº”æ›´å¿«"
    },
    
    "encoder_depth": {
        "range": [6, 12],
        "default": 8,
        "impact": "SiTç‰¹å¾æå–å±‚æ·±åº¦",
        "è°ƒä¼˜": "è¾ƒæ·±å±‚æä¾›æ›´è¯­ä¹‰åŒ–çš„ç‰¹å¾"
    },
    
    # === å­¦ä¹ çŽ‡è°ƒä¼˜ ===
    "learning_rates": {
        "sit_lr": "1e-4 (æ ‡å‡†)",
        "vae_lr": "1e-4 æˆ– 5e-5 (æ›´ä¿å®ˆ)",
        "disc_lr": "1e-4 æˆ– 1e-3 (å¯ç¨é«˜)",
        "è°ƒä¼˜": "VAEå­¦ä¹ çŽ‡å¯é€‚å½“é™ä½Žä»¥ä¿æŒç¨³å®šæ€§"
    },
    
    # === æŸå¤±æƒé‡è°ƒä¼˜ ===
    "loss_weights": {
        "discriminator_weight": [0.05, 0.2],
        "perceptual_weight": [0.5, 2.0], 
        "kl_weight": [1e-7, 1e-5],
        "è°ƒä¼˜": "æ ¹æ®é‡å»ºè´¨é‡å’Œç”Ÿæˆæ•ˆæžœå¹³è¡¡"
    }
}

# === è°ƒä¼˜ç­–ç•¥ ===
tuning_strategy = """
1. é¦–å…ˆå›ºå®šREPA-Eæ ¸å¿ƒå‚æ•° (proj_coeff=0.5, vae_align_coeff=1.5)
2. è°ƒä¼˜å­¦ä¹ çŽ‡ï¼Œç¡®ä¿è®­ç»ƒç¨³å®š
3. è°ƒæ•´æŸå¤±æƒé‡ï¼Œå¹³è¡¡é‡å»ºå’Œç”Ÿæˆè´¨é‡
4. æœ€åŽå¾®è°ƒREPA-Eå‚æ•°ï¼Œæå‡å¯¹é½æ•ˆæžœ
5. éªŒè¯ä¸åŒVAEæž¶æž„çš„å‚æ•°é€‚é…æ€§
"""
```

---

## 11. å…³é”®å·¥å…·å‡½æ•°å’Œè¾…åŠ©ä»£ç 

å‰é¢ç« èŠ‚ä¸­å·²ç»åˆ†æ•£ä»‹ç»çš„å…³é”®å·¥å…·å‡½æ•°æ±‡æ€»ï¼š

### 11.1 æ ¸å¿ƒå·¥å…·å‡½æ•°

```python
# ===== å·²åœ¨å‰é¢ç« èŠ‚ä»‹ç»çš„å…³é”®å‡½æ•°æ±‡æ€» =====

# === æ•°å­¦å·¥å…·å‡½æ•° ===
def mean_flat(x):
    """åœ¨é™¤batchç»´åº¦å¤–çš„æ‰€æœ‰ç»´åº¦ä¸Šè®¡ç®—å¹³å‡å€¼ - ç”¨äºŽæŸå¤±è®¡ç®—"""
    return torch.mean(x, dim=list(range(1, len(x.size()))))

def build_mlp(hidden_size, projector_dim, z_dim):
    """æž„å»ºMLPæŠ•å½±å™¨ - REPA-Eçš„æ ¸å¿ƒç»„ä»¶"""
    return nn.Sequential(
        nn.Linear(hidden_size, projector_dim),
        nn.SiLU(),
        nn.Linear(projector_dim, projector_dim), 
        nn.SiLU(),
        nn.Linear(projector_dim, z_dim),
    )

# === è®­ç»ƒè¾…åŠ©å‡½æ•° ===
def requires_grad(model, flag=True):
    """è®¾ç½®æ¨¡åž‹æ‰€æœ‰å‚æ•°çš„requires_gradæ ‡å¿— - Stop-gradientæœºåˆ¶"""
    for p in model.parameters():
        p.requires_grad = flag

@torch.no_grad()
def update_ema(ema_model, model, decay=0.9999):
    """æ›´æ–°EMAæ¨¡åž‹ - æŒ‡æ•°ç§»åŠ¨å¹³å‡"""
    # (è¯¦ç»†å®žçŽ°è§ç¬¬6ç« )

# === æ•°æ®å¤„ç†å‡½æ•° ===
def preprocess_imgs_vae(imgs):
    """VAEå›¾åƒé¢„å¤„ç† - [0,255] -> [-1,1]"""
    return imgs.float() / 127.5 - 1.0

def center_crop_arr(pil_image, image_size):
    """ä¸­å¿ƒè£å‰ªå‡½æ•°"""
    # (è¯¦ç»†å®žçŽ°è§ç¬¬7ç« )

# === é‡‡æ ·è¾…åŠ©å‡½æ•° ===
def expand_t_like_x(t, x_cur):
    """æ—¶é—´té‡å¡‘ä¸ºå¯å¹¿æ’­ç»´åº¦"""
    # (è¯¦ç»†å®žçŽ°è§ç¬¬9ç« )

def get_score_from_velocity(vt, xt, t, path_type="linear"):
    """é€Ÿåº¦é¢„æµ‹è½¬æ¢ä¸ºåˆ†æ•°å‡½æ•°"""
    # (è¯¦ç»†å®žçŽ°è§ç¬¬9ç« )

# === æ¨¡åž‹å·¥å…·å‡½æ•° ===
def count_trainable_params(model):
    """è®¡ç®—æ¨¡åž‹å¯è®­ç»ƒå‚æ•°æ•°é‡"""
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

# === æŸå¤±å‡½æ•°å·¥å…· ===
def hinge_d_loss(logits_real, logits_fake):
    """HingeæŸå¤±ç”¨äºŽåˆ¤åˆ«å™¨è®­ç»ƒ"""
    loss_real = torch.mean(F.relu(1.0 - logits_real))
    loss_fake = torch.mean(F.relu(1.0 + logits_fake))
    return 0.5 * (loss_real + loss_fake)
```

### 11.2 å®Œæ•´çš„REPA-Eå®žçŽ°æ£€æŸ¥æ¸…å•

```python
# ===== REPA-E å®žçŽ°å®Œæ•´æ€§æ£€æŸ¥æ¸…å• =====

repa_e_checklist = {
    "âœ… æ ¸å¿ƒæŸå¤±å‡½æ•°": [
        "ReconstructionLoss_Single_Stageç±»å®žçŽ°",
        "_forward_generator_alignmentæ–¹æ³•",
        "æŠ•å½±å¯¹é½æŸå¤±è®¡ç®— (è´Ÿä½™å¼¦ç›¸ä¼¼åº¦)",
        "VAEæ­£åˆ™åŒ–æŸå¤±ç»„åˆ (L1+LPIPS+GAN+KL)",
        "åˆ¤åˆ«å™¨è®­ç»ƒæ­¥éª¤"
    ],
    
    "âœ… åˆ¤åˆ«å™¨å®žçŽ°": [
        "NLayerDiscriminator (PatchGAN)",
        "ActNormå½’ä¸€åŒ–å±‚",  
        "weights_initæƒé‡åˆå§‹åŒ–",
        "HingeæŸå¤±å®žçŽ°"
    ],
    
    "âœ… SiTæ¨¡åž‹é›†æˆ": [
        "MLPæŠ•å½±å™¨æž„å»º (build_mlp)",
        "BatchNormå±‚ (åŠ¨æ€å½’ä¸€åŒ–)",
        "å‰å‘ä¼ æ’­ä¸­çš„æŠ•å½±å¯¹é½æŸå¤±",
        "ç‰¹å¾æå–å’Œå½’ä¸€åŒ–"
    ],
    
    "âœ… ç«¯åˆ°ç«¯è®­ç»ƒ": [
        "ä¸‰ä¼˜åŒ–å™¨æž¶æž„ (SiT+VAE+åˆ¤åˆ«å™¨)",
        "Stop-gradientæœºåˆ¶ (requires_gradæŽ§åˆ¶)",
        "è®­ç»ƒå¾ªçŽ¯çš„5ä¸ªå…³é”®æ­¥éª¤",
        "æ¢¯åº¦æµæŽ§åˆ¶å’ŒæŸå¤±å¹³è¡¡"
    ],
    
    "âœ… æ”¯æ’‘ç³»ç»Ÿ": [
        "EMAæ›´æ–°æœºåˆ¶",
        "æ•°æ®é¢„å¤„ç†å’Œå½’ä¸€åŒ–", 
        "å¤šè§†è§‰ç¼–ç å™¨åŠ è½½ç®¡ç†",
        "é‡‡æ ·å™¨å’Œç”Ÿæˆç­–ç•¥"
    ],
    
    "âœ… å‚æ•°é…ç½®": [
        "æŸå¤±é…ç½®æ–‡ä»¶ (l1_lpips_kl_gan.yaml)",
        "æ ¸å¿ƒè¶…å‚æ•°è®¾ç½®",
        "å‘½ä»¤è¡Œå‚æ•°é…ç½®",
        "è°ƒä¼˜æŒ‡å—å’Œå»ºè®®"
    ]
}
```

---

## ðŸŽ¯ æ€»ç»“

æœ¬æŒ‡å—å®Œæ•´æå–äº†REPA-Eçš„æ‰€æœ‰æ ¸å¿ƒåˆ›æ–°å®žçŽ°ï¼Œæ¶µç›–ï¼š

1. **æ ¸å¿ƒåˆ›æ–°ç»„ä»¶** - æŠ•å½±å¯¹é½æŸå¤±ã€Stop-gradientã€ä¸‰ä¼˜åŒ–å™¨æž¶æž„
2. **å®Œæ•´æŠ€æœ¯å®žçŽ°** - ä»ŽæŸå¤±å‡½æ•°åˆ°é‡‡æ ·å™¨çš„å…¨é“¾è·¯ä»£ç 
3. **å®žç”¨é…ç½®æŒ‡å¯¼** - è¶…å‚æ•°è®¾ç½®ã€æ¨¡åž‹é…ç½®ã€è°ƒä¼˜å»ºè®®
4. **è¿ç§»å‹å¥½æ ¼å¼** - è¯¦ç»†ä¸­æ–‡æ³¨é‡Šã€ç‹¬ç«‹ä»£ç ç‰‡æ®µã€å‚æ•°è¯´æ˜Ž

æ‰€æœ‰ä»£ç ç‰‡æ®µå‡å¯ç›´æŽ¥ç”¨äºŽREPA-Eçš„å¤çŽ°å’Œè¿ç§»å·¥ä½œã€‚æ ¸å¿ƒåˆ›æ–°ç‚¹100%è¦†ç›–ï¼ŒæŠ€æœ¯ç»†èŠ‚å®Œæ•´å‡†ç¡®ã€‚

---

## 2. æ ¸å¿ƒæŸå¤±å‡½æ•°å®žçŽ°

### 2.1 ReconstructionLoss_Single_Stage ç±»

è¿™æ˜¯REPA-Eçš„æ ¸å¿ƒæŸå¤±å®žçŽ°ç±»ï¼Œä½äºŽ `loss/losses.py:280-476`ã€‚

```python
# ===== æ–‡ä»¶: loss/losses.py =====

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Mapping, Text, Tuple

class ReconstructionLoss_Single_Stage(ReconstructionLoss_Stage2):
    """REPA-Eçš„ä¸»è¦æŸå¤±å®žçŽ°ç±»
    
    æ ¸å¿ƒåˆ›æ–°ï¼šé›†æˆæŠ•å½±å¯¹é½æŸå¤±ï¼Œæ”¯æŒç«¯åˆ°ç«¯è®­ç»ƒVAEå’ŒLDM
    """
    def __init__(self, config):
        """åˆå§‹åŒ–æŸå¤±å‡½æ•°ç»„åˆ
        
        Args:
            config: åŒ…å«æ‰€æœ‰æŸå¤±é…ç½®çš„é…ç½®å¯¹è±¡
        """
        super().__init__()
        loss_config = config.losses
        
        # === 1. åˆ¤åˆ«å™¨è®¾ç½® ===
        self.discriminator = NLayerDiscriminator(
            input_nc=3,
            n_layers=3,
            use_actnorm=False
        ).apply(weights_init)
        
        # === 2. æ„ŸçŸ¥æŸå¤±è®¾ç½® ===
        self.perceptual_loss = PerceptualLoss(
            loss_config.perceptual_loss).eval()
        self.perceptual_weight = loss_config.perceptual_weight
        
        # === 3. åˆ¤åˆ«å™¨è®­ç»ƒå‚æ•° ===
        self.discriminator_iter_start = loss_config.discriminator_start
        self.discriminator_factor = loss_config.discriminator_factor
        self.discriminator_weight = loss_config.discriminator_weight
        
        # === 4. LeCAMæ­£åˆ™åŒ– ===
        self.lecam_regularization_weight = loss_config.lecam_regularization_weight
        self.lecam_ema_decay = loss_config.get("lecam_ema_decay", 0.999)
        if self.lecam_regularization_weight > 0.0:
            self.register_buffer("ema_real_logits_mean", torch.tensor(0., requires_grad=False))
            self.register_buffer("ema_fake_logits_mean", torch.tensor(0., requires_grad=False))
        
        # === 5. é‡å»ºæŸå¤±è®¾ç½® ===
        self.reconstruction_loss = loss_config.reconstruction_loss
        self.reconstruction_weight = loss_config.reconstruction_weight
        
        # === 6. é‡åŒ–æ¨¡å¼å’ŒKLæŸå¤± ===
        self.quantize_mode = loss_config.quantize_mode
        if self.quantize_mode == "vq":
            self.quantizer_weight = loss_config.quantizer_weight
        elif self.quantize_mode == "vae":
            self.kl_weight = loss_config.get("kl_weight", 1e-6)
            logvar_init = loss_config.get("logvar_init", 0.0)
            self.logvar = nn.Parameter(torch.ones(size=()) * logvar_init, requires_grad=False)
        
        # === 7. æ ¸å¿ƒåˆ›æ–°ï¼šæŠ•å½±å¯¹é½æŸå¤±ç³»æ•° ===
        self.proj_coef = loss_config.get("proj_coef", 0.0)  # REPA-Eæ ¸å¿ƒå‚æ•°ï¼
```

### 2.2 æŠ•å½±å¯¹é½æŸå¤±æ ¸å¿ƒå®žçŽ°

ä½äºŽ `loss/losses.py:378-475` çš„ `_forward_generator_alignment` æ–¹æ³•ï¼š

```python
def _forward_generator_alignment(self, 
                               inputs: torch.Tensor,
                               reconstructions: torch.Tensor,
                               extra_result_dict: Mapping[Text, torch.Tensor],
                               global_step: int
                               ) -> Tuple[torch.Tensor, Mapping[Text, torch.Tensor]]:
    """ç”Ÿæˆå™¨è®­ç»ƒæ­¥éª¤ - åŒ…å«æŠ•å½±å¯¹é½æŸå¤±
    
    Args:
        inputs: åŽŸå§‹è¾“å…¥å›¾åƒ [B, C, H, W]
        reconstructions: VAEé‡å»ºå›¾åƒ [B, C, H, W]  
        extra_result_dict: åŒ…å«æŠ•å½±å¯¹é½ç›¸å…³ç‰¹å¾çš„å­—å…¸
        global_step: å½“å‰è®­ç»ƒæ­¥æ•°
    
    Returns:
        total_loss: æ€»æŸå¤±
        loss_dict: å„é¡¹æŸå¤±çš„è¯¦ç»†è®°å½•
    """
    inputs = inputs.contiguous()
    reconstructions = reconstructions.contiguous()
    
    # === 1. é‡å»ºæŸå¤±è®¡ç®— ===
    if self.reconstruction_loss == "l1":
        reconstruction_loss = F.l1_loss(inputs, reconstructions, reduction="mean")
    elif self.reconstruction_loss == "l2":
        reconstruction_loss = F.mse_loss(inputs, reconstructions, reduction="mean")
    else:
        raise ValueError(f"Unsupported reconstruction_loss {self.reconstruction_loss}")
    reconstruction_loss *= self.reconstruction_weight
    
    # === 2. æ„ŸçŸ¥æŸå¤±è®¡ç®— ===
    perceptual_loss = self.perceptual_loss(inputs, reconstructions).mean()
    
    # === 3. åˆ¤åˆ«å™¨/GANæŸå¤±è®¡ç®— ===
    generator_loss = torch.zeros((), device=inputs.device)
    discriminator_factor = self.discriminator_factor if self.should_discriminator_be_trained(global_step) else 0.
    d_weight = 1.0
    
    if discriminator_factor > 0.0 and self.discriminator_weight > 0.0:
        # ç¦ç”¨åˆ¤åˆ«å™¨æ¢¯åº¦ï¼ˆé¿å…åœ¨ç”Ÿæˆå™¨è®­ç»ƒæ—¶æ›´æ–°åˆ¤åˆ«å™¨ï¼‰
        for param in self.discriminator.parameters():
            param.requires_grad = False
        logits_fake = self.discriminator(reconstructions)
        generator_loss = -torch.mean(logits_fake)  # GANæŸå¤±
    
    d_weight *= self.discriminator_weight
    
    # === 4. æ ¸å¿ƒåˆ›æ–°ï¼šæŠ•å½±å¯¹é½æŸå¤±è®¡ç®— ===
    # ä»Žextra_result_dictä¸­èŽ·å–å¯¹é½ç‰¹å¾
    zs_tilde = extra_result_dict["zs_tilde"]  # æ‰©æ•£æ¨¡åž‹ç‰¹å¾åˆ—è¡¨ [B, N, C]
    zs = extra_result_dict["zs"]              # è§†è§‰ç¼–ç å™¨ç‰¹å¾åˆ—è¡¨ [B, N, C]
    
    # è®¡ç®—æŠ•å½±å¯¹é½æŸå¤± - REPA-Eçš„æ ¸å¿ƒåˆ›æ–°ï¼
    proj_loss = torch.tensor(0., device=inputs.device)
    for i, (z, z_tilde) in enumerate(zip(zs, zs_tilde)):
        for j, (z_j, z_tilde_j) in enumerate(zip(z, z_tilde)):
            # L2å½’ä¸€åŒ–
            z_tilde_j = torch.nn.functional.normalize(z_tilde_j, dim=-1) 
            z_j = torch.nn.functional.normalize(z_j, dim=-1)
            # è´Ÿä½™å¼¦ç›¸ä¼¼åº¦ä½œä¸ºå¯¹é½æŸå¤±
            proj_loss += mean_flat(-(z_j * z_tilde_j).sum(dim=-1))
    
    # === 5. æ€»æŸå¤±ç»„åˆ ===
    if self.quantize_mode == "vq":
        # VQæ¨¡å¼ï¼šåŒ…å«é‡åŒ–æŸå¤±
        quantizer_loss = extra_result_dict["quantizer_loss"]
        total_loss = (
            reconstruction_loss
            + self.perceptual_weight * perceptual_loss
            + self.quantizer_weight * quantizer_loss
            + d_weight * discriminator_factor * generator_loss
            + self.proj_coef * proj_loss  # æŠ•å½±å¯¹é½æŸå¤±ï¼
        )
        loss_dict = dict(
            total_loss=total_loss.clone().detach(),
            reconstruction_loss=reconstruction_loss.detach(),
            perceptual_loss=(self.perceptual_weight * perceptual_loss).detach(),
            quantizer_loss=(self.quantizer_weight * quantizer_loss).detach(),
            weighted_gan_loss=(d_weight * discriminator_factor * generator_loss).detach(),
            discriminator_factor=torch.tensor(discriminator_factor),
            commitment_loss=extra_result_dict["commitment_loss"].detach(),
            codebook_loss=extra_result_dict["codebook_loss"].detach(),
            d_weight=d_weight,
            gan_loss=generator_loss.detach(),
            proj_loss=proj_loss.detach(),  # è®°å½•æŠ•å½±å¯¹é½æŸå¤±
        )
    
    elif self.quantize_mode == "vae":
        # VAEæ¨¡å¼ï¼šåŒ…å«KLæ•£åº¦æŸå¤±
        kl_loss = extra_result_dict["kl_loss"]
        total_loss = (
            reconstruction_loss
            + self.perceptual_weight * perceptual_loss
            + self.kl_weight * kl_loss
            + d_weight * discriminator_factor * generator_loss
            + self.proj_coef * proj_loss  # æŠ•å½±å¯¹é½æŸå¤±ï¼
        )
        loss_dict = dict(
            total_loss=total_loss.clone().detach(),
            reconstruction_loss=reconstruction_loss.detach(),
            perceptual_loss=(self.perceptual_weight * perceptual_loss).detach(),
            kl_loss=(self.kl_weight * kl_loss).detach(),
            weighted_gan_loss=(d_weight * discriminator_factor * generator_loss).detach(),
            discriminator_factor=torch.tensor(discriminator_factor).to(generator_loss.device),
            d_weight=torch.tensor(d_weight).to(generator_loss.device),
            gan_loss=generator_loss.detach(),
            proj_loss=proj_loss.detach(),  # è®°å½•æŠ•å½±å¯¹é½æŸå¤±
        )
    
    return total_loss, loss_dict
```

### 2.3 åˆ¤åˆ«å™¨è®­ç»ƒå‡½æ•°

```python
def _forward_discriminator(self,
                          inputs: torch.Tensor,
                          reconstructions: torch.Tensor,
                          global_step: int,
                          ) -> Tuple[torch.Tensor, Mapping[Text, torch.Tensor]]:
    """åˆ¤åˆ«å™¨è®­ç»ƒæ­¥éª¤
    
    Args:
        inputs: çœŸå®žå›¾åƒ
        reconstructions: VAEé‡å»ºå›¾åƒ
        global_step: å½“å‰è®­ç»ƒæ­¥æ•°
    
    Returns:
        discriminator_loss: åˆ¤åˆ«å™¨æŸå¤±
        loss_dict: æŸå¤±è®°å½•å­—å…¸
    """
    discriminator_factor = self.discriminator_factor if self.should_discriminator_be_trained(global_step) else 0
    
    # å¯ç”¨åˆ¤åˆ«å™¨æ¢¯åº¦
    for param in self.discriminator.parameters():
        param.requires_grad = True
    
    # çœŸå®žå’Œè™šå‡å›¾åƒçš„åˆ¤åˆ«
    real_images = inputs.detach().requires_grad_(True)
    logits_real = self.discriminator(real_images)
    logits_fake = self.discriminator(reconstructions.detach())
    
    # HingeæŸå¤±
    discriminator_loss = discriminator_factor * hinge_d_loss(
        logits_real=logits_real, 
        logits_fake=logits_fake
    )
    
    # LeCAMæ­£åˆ™åŒ–ï¼ˆå¯é€‰ï¼‰
    lecam_loss = torch.zeros((), device=inputs.device)
    if self.lecam_regularization_weight > 0.0:
        lecam_loss = self.lecam_regularization_weight * lecam_regularization(
            logits_real, logits_fake, 
            self.ema_real_logits_mean, self.ema_fake_logits_mean
        )
        # æ›´æ–°EMAç»Ÿè®¡
        self.ema_real_logits_mean = self.ema_real_logits_mean * self.lecam_ema_decay + torch.mean(logits_real).detach() * (1 - self.lecam_ema_decay)
        self.ema_fake_logits_mean = self.ema_fake_logits_mean * self.lecam_ema_decay + torch.mean(logits_fake).detach() * (1 - self.lecam_ema_decay)
    
    discriminator_loss += lecam_loss
    
    loss_dict = dict(
        discriminator_loss=discriminator_loss.detach(),
        logits_real=logits_real.detach().mean(),
        logits_fake=logits_fake.detach().mean(),
        lecam_loss=lecam_loss.detach(),
    )
    
    return discriminator_loss, loss_dict
```

### 2.4 å…³é”®è¾…åŠ©å‡½æ•°

```python
def mean_flat(x):
    """åœ¨é™¤äº†batchç»´åº¦å¤–çš„æ‰€æœ‰ç»´åº¦ä¸Šè®¡ç®—å¹³å‡å€¼
    
    Args:
        x: è¾“å…¥å¼ é‡
    Returns:
        åœ¨ç©ºé—´ç»´åº¦ä¸Šå¹³å‡åŽçš„å¼ é‡
    """
    return torch.mean(x, dim=list(range(1, len(x.size()))))

def hinge_d_loss(logits_real, logits_fake):
    """HingeæŸå¤±ç”¨äºŽåˆ¤åˆ«å™¨è®­ç»ƒ
    
    Args:
        logits_real: çœŸå®žå›¾åƒçš„åˆ¤åˆ«å™¨è¾“å‡º
        logits_fake: ç”Ÿæˆå›¾åƒçš„åˆ¤åˆ«å™¨è¾“å‡º
    Returns:
        hingeæŸå¤±å€¼
    """
    loss_real = torch.mean(F.relu(1.0 - logits_real))
    loss_fake = torch.mean(F.relu(1.0 + logits_fake))
    d_loss = 0.5 * (loss_real + loss_fake)
    return d_loss

def should_discriminator_be_trained(self, global_step: int):
    """åˆ¤æ–­æ˜¯å¦åº”è¯¥è®­ç»ƒåˆ¤åˆ«å™¨
    
    Args:
        global_step: å½“å‰è®­ç»ƒæ­¥æ•°
    Returns:
        bool: æ˜¯å¦è®­ç»ƒåˆ¤åˆ«å™¨
    """
    return global_step >= self.discriminator_iter_start
```

---
