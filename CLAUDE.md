# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## é¡¹ç›®æ¦‚è¿°
REPA-Eæ˜¯ä¸€ä¸ªç”¨äºç«¯åˆ°ç«¯è®­ç»ƒæ½œåœ¨æ‰©æ•£å˜æ¢å™¨çš„VAEæ¨¡å‹ã€‚å®ƒå®ç°äº†VAEä¸æ‰©æ•£æ¨¡å‹çš„è”åˆè®­ç»ƒï¼Œé€šè¿‡è¡¨ç¤ºå¯¹é½(REPA)æŸå¤±å®ç°ç¨³å®šæœ‰æ•ˆçš„è®­ç»ƒã€‚

## æ ¸å¿ƒåˆ›æ–°ç‚¹å’Œå…·ä½“å®ç°ä½ç½®

### ğŸ¯ æ ¸å¿ƒåˆ›æ–°ï¼šæŠ•å½±å¯¹é½æŸå¤±ï¼ˆProjection Alignment Lossï¼‰

**ä¸»è¦åˆ›æ–°**ï¼šREPA-Eçš„æ ¸å¿ƒåˆ›æ–°æ˜¯é€šè¿‡æŠ•å½±å¯¹é½æŸå¤±å®ç°VAEå’Œæ‰©æ•£æ¨¡å‹çš„ç«¯åˆ°ç«¯è®­ç»ƒã€‚

#### å…³é”®å®ç°ä½ç½®ï¼š

1. **æŠ•å½±å¯¹é½æŸå¤±è®¡ç®—** - `loss/losses.py:411-422`ï¼š
   ```python
   proj_loss = torch.tensor(0., device=inputs.device)
   for i, (z, z_tilde) in enumerate(zip(zs, zs_tilde)):
       for j, (z_j, z_tilde_j) in enumerate(zip(z, z_tilde)):
           z_tilde_j = torch.nn.functional.normalize(z_tilde_j, dim=-1) 
           z_j = torch.nn.functional.normalize(z_j, dim=-1) 
           proj_loss += mean_flat(-(z_j * z_tilde_j).sum(dim=-1))
   ```

2. **SiTæ¨¡å‹ä¸­çš„æŠ•å½±å¯¹é½** - `models/sit.py:372-379`ï¼š
   ```python
   proj_loss = torch.tensor(0., device=x.device)
   for i, (z, z_tilde) in enumerate(zip(zs, zs_tilde)):
       for z_j, z_tilde_j in zip(z, z_tilde):
           z_tilde_j = torch.nn.functional.normalize(z_tilde_j, dim=-1) 
           z_j = torch.nn.functional.normalize(z_j, dim=-1) 
           proj_loss += mean_flat(-(z_j * z_tilde_j).sum(dim=-1))
   ```

3. **ç«¯åˆ°ç«¯è®­ç»ƒå¾ªç¯** - `train_repae.py:379-432`ï¼š
   - VAEå¯¹é½æŸå¤±ï¼š`ç¬¬389è¡Œ`
   - SiTå¯¹é½æŸå¤±ï¼š`ç¬¬427è¡Œ`

### ğŸ”§ æ ¸å¿ƒæ–‡ä»¶ç»“æ„è¯¦è§£

#### ä¸»è¦è®­ç»ƒè„šæœ¬
- **`train_repae.py`** - REPA-Eç«¯åˆ°ç«¯è®­ç»ƒçš„æ ¸å¿ƒè„šæœ¬
  - ç¬¬379-398è¡Œï¼šVAEè®­ç»ƒå’Œå¯¹é½æŸå¤±è®¡ç®—
  - ç¬¬413-432è¡Œï¼šSiTè®­ç»ƒå’ŒæŠ•å½±å¯¹é½æŸå¤±
  - ç¬¬225-246è¡Œï¼šä¸‰ä¸ªç‹¬ç«‹ä¼˜åŒ–å™¨ï¼ˆSiTã€VAEã€åˆ¤åˆ«å™¨ï¼‰

- **`train_ldm_only.py`** - ä»…è®­ç»ƒæ‰©æ•£æ¨¡å‹ï¼ˆå›ºå®šVAEï¼‰çš„ä¼ ç»Ÿæ–¹æ³•
- **`generate.py`** - æ ·æœ¬ç”Ÿæˆå’Œè¯„ä¼°è„šæœ¬

#### æ¨¡å‹æ¶æ„å®ç°
- **`models/sit.py`** - SiTï¼ˆScalable Interpolant Transformersï¼‰æ‰©æ•£æ¨¡å‹
  - ç¬¬308-388è¡Œï¼šé›†æˆæŸå¤±å‡½æ•°è®¡ç®—çš„å‰å‘ä¼ æ’­
  - ç¬¬362-365è¡Œï¼šæŠ•å½±å™¨å’Œå¯¹é½ç‰¹å¾æå–
  - ç¬¬23-30è¡Œï¼šMLPæŠ•å½±å™¨æ„å»º

- **`models/autoencoder.py`** - VAEæ¨¡å‹å®ç°ï¼Œæ”¯æŒf8d4å’Œf16d32æ¶æ„
- **`models/`** - åŒ…å«å¤šç§è§†è§‰ç¼–ç å™¨ï¼ˆCLIPã€DINOv2ã€MAEã€MoCov3ã€JEPAï¼‰

#### æŸå¤±å‡½æ•°æ ¸å¿ƒ
- **`loss/losses.py`** - æŸå¤±å‡½æ•°é›†åˆ
  - ç¬¬280-476è¡Œï¼š`ReconstructionLoss_Single_Stage`ç±» - REPA-Eçš„ä¸»è¦æŸå¤±å®ç°
  - ç¬¬378-475è¡Œï¼š`_forward_generator_alignment`æ–¹æ³• - æŠ•å½±å¯¹é½æŸå¤±çš„æ ¸å¿ƒå®ç°
  - ç¬¬294-295è¡Œï¼šæŠ•å½±ç³»æ•°å‚æ•°å®šä¹‰

- **`loss/perceptual_loss.py`** - æ„ŸçŸ¥æŸå¤±ï¼ˆLPIPSï¼‰å®ç°  
- **`loss/discriminator.py`** - GANåˆ¤åˆ«å™¨å®ç°

#### æ•°æ®å’Œå·¥å…·
- **`dataset.py`** - ImageNet-1Kæ•°æ®é›†å¤„ç†
- **`preprocessing.py`** - æ•°æ®é¢„å¤„ç†è„šæœ¬
- **`utils.py`** - å·¥å…·å‡½æ•°ï¼ŒåŒ…å«ç¼–ç å™¨åŠ è½½ã€ç‰¹å¾å½’ä¸€åŒ–ç­‰
- **`samplers.py`** - æ‰©æ•£é‡‡æ ·ç­–ç•¥å®ç°

#### è¾…åŠ©è„šæœ¬
- **`cache_latents.py`** - E2E-VAEæ½œåœ¨è¡¨ç¤ºç¼“å­˜ï¼Œç”¨äºåŠ é€Ÿè®­ç»ƒ
- **`save_vae_weights.py`** - ä»REPA-Eæ£€æŸ¥ç‚¹æå–VAEæƒé‡

## ç¯å¢ƒè®¾ç½®
```bash
conda env create -f environment.yml -y
conda activate repa-e
```

## ä¸»è¦è®­ç»ƒå‘½ä»¤

### 1. æ•°æ®é¢„å¤„ç†
```bash
python preprocessing.py --imagenet-path /PATH/TO/IMAGENET_TRAIN
```

### 2. REPA-Eç«¯åˆ°ç«¯è®­ç»ƒï¼ˆæ ¸å¿ƒåˆ›æ–°ï¼‰
```bash
accelerate launch train_repae.py \
    --max-train-steps=400000 \
    --report-to="wandb" \
    --allow-tf32 \
    --mixed-precision="fp16" \
    --seed=0 \
    --data-dir="data" \
    --output-dir="exps" \
    --batch-size=256 \
    --path-type="linear" \
    --prediction="v" \
    --weighting="uniform" \
    --model="SiT-XL/2" \
    --checkpointing-steps=50000 \
    --loss-cfg-path="configs/l1_lpips_kl_gan.yaml" \
    --vae="f8d4" \
    --vae-ckpt="pretrained/sdvae/sdvae-f8d4.pt" \
    --disc-pretrained-ckpt="pretrained/sdvae/sdvae-f8d4-discriminator-ckpt.pt" \
    --enc-type="dinov2-vit-b" \
    --proj-coeff=0.5 \
    --encoder-depth=8 \
    --vae-align-proj-coeff=1.5 \
    --bn-momentum=0.1 \
    --exp-name="sit-xl-dinov2-b-enc8-repae-sdvae-0.5-1.5-400k"
```

### 3. E2E-VAEç¼“å­˜æ½œåœ¨è¡¨ç¤º
```bash
accelerate launch --num_machines=1 --num_processes=8 cache_latents.py \
    --vae-arch="f16d32" \
    --vae-ckpt-path="pretrained/e2e-vavae/e2e-vavae-400k.pt" \
    --vae-latents-name="e2e-vavae" \
    --pproc-batch-size=128
```

### 4. ä¼ ç»Ÿæ½œåœ¨æ‰©æ•£æ¨¡å‹è®­ç»ƒï¼ˆå›ºå®šVAEï¼‰
```bash
accelerate launch train_ldm_only.py \
    --max-train-steps=4000000 \
    --report-to="wandb" \
    --allow-tf32 \
    --mixed-precision="fp16" \
    --seed=0 \
    --data-dir="data" \
    --batch-size=256 \
    --path-type="linear" \
    --prediction="v" \
    --weighting="uniform" \
    --model="SiT-XL/1" \
    --checkpointing-steps=50000 \
    --vae="f16d32" \
    --vae-ckpt="pretrained/e2e-vavae/e2e-vavae-400k.pt" \
    --vae-latents-name="e2e-vavae" \
    --learning-rate=1e-4 \
    --output-dir="exps" \
    --exp-name="sit-xl-1-dinov2-b-enc8-ldm-only-e2e-vavae-0.5-4m"
```

### 5. æ ·æœ¬ç”Ÿæˆå’Œè¯„ä¼°
```bash
torchrun --nnodes=1 --nproc_per_node=8 generate.py \
    --num-fid-samples 50000 \
    --path-type linear \
    --mode sde \
    --num-steps 250 \
    --cfg-scale 1.0 \
    --guidance-high 1.0 \
    --guidance-low 0.0 \
    --exp-path pretrained/sit-ldm-e2e-vavae \
    --train-steps 4000000
```

### 6. ä»REPA-Eæ£€æŸ¥ç‚¹æå–VAEæƒé‡
```bash
python save_vae_weights.py \
    --repae-ckpt pretrained/sit-repae-vavae/checkpoints/0400000.pt \
    --vae-name e2e-vavae \
    --save-dir exps
```

## æ ¸å¿ƒæŠ€æœ¯æ¶æ„

### REPA-Eè®­ç»ƒæµç¨‹
1. **VAEè®­ç»ƒé˜¶æ®µ**ï¼šè®¡ç®—é‡å»ºæŸå¤±ã€æ„ŸçŸ¥æŸå¤±ã€KLæŸå¤±å’ŒVAEå¯¹é½æŸå¤±
2. **åˆ¤åˆ«å™¨è®­ç»ƒ**ï¼šæ›´æ–°GANåˆ¤åˆ«å™¨
3. **SiTè®­ç»ƒé˜¶æ®µ**ï¼šè®¡ç®—å»å™ªæŸå¤±å’ŒSiTæŠ•å½±å¯¹é½æŸå¤±
4. **EMAæ›´æ–°**ï¼šæ›´æ–°SiTçš„æŒ‡æ•°ç§»åŠ¨å¹³å‡å‚æ•°

### æŸå¤±å‡½æ•°ç»„åˆ
- **é‡å»ºæŸå¤±**: L1æŸå¤±ç”¨äºåƒç´ çº§é‡å»º
- **æ„ŸçŸ¥æŸå¤±**: LPIPSæŸå¤±ç”¨äºæ„ŸçŸ¥è´¨é‡  
- **REPAæŸå¤±**: è¡¨ç¤ºå¯¹é½æŸå¤±ï¼ˆæ ¸å¿ƒåˆ›æ–°ï¼‰
- **åˆ¤åˆ«å™¨æŸå¤±**: GANæŸå¤±ç”¨äºæå‡ç”Ÿæˆè´¨é‡
- **KLæ•£åº¦**: ç”¨äºVAEæ­£åˆ™åŒ–

### æ”¯æŒçš„æ¨¡å‹é…ç½®
- **VAEæ¶æ„**: f8d4ï¼ˆ8å€ä¸‹é‡‡æ ·ï¼Œ4é€šé“ï¼‰ã€f16d32ï¼ˆ16å€ä¸‹é‡‡æ ·ï¼Œ32é€šé“ï¼‰
- **SiTæ¨¡å‹**: SiT-B/L/XLï¼Œpatch size 1/2
- **è§†è§‰ç¼–ç å™¨**: DINOv2ã€DINOv1ã€CLIPã€MoCov3ã€MAEã€JEPA

### æ ¸å¿ƒå‚æ•°
- `--proj-coeff`: SiTæŠ•å½±å¯¹é½æŸå¤±ç³»æ•°ï¼ˆé€šå¸¸0.5ï¼‰
- `--vae-align-proj-coeff`: VAEæŠ•å½±å¯¹é½æŸå¤±ç³»æ•°ï¼ˆé€šå¸¸1.5ï¼‰
- `--encoder-depth`: ç¼–ç å™¨æ·±åº¦ï¼Œæ§åˆ¶ç‰¹å¾æå–å±‚æ•°

## é¢„è®­ç»ƒæ¨¡å‹ç›®å½•ç»“æ„
```
pretrained/
â”œâ”€â”€ sdvae/           # SD-VAEæ¨¡å‹
â”œâ”€â”€ invae/           # IN-VAEæ¨¡å‹  
â”œâ”€â”€ vavae/           # VA-VAEæ¨¡å‹
â”œâ”€â”€ e2e-sdvae/       # E2Eè°ƒä¼˜çš„SD-VAE
â”œâ”€â”€ e2e-invae/       # E2Eè°ƒä¼˜çš„IN-VAE
â””â”€â”€ e2e-vavae/       # E2Eè°ƒä¼˜çš„VA-VAE

data/                # é¢„å¤„ç†åçš„è®­ç»ƒæ•°æ®
exps/                # å®éªŒè¾“å‡ºç›®å½•
```